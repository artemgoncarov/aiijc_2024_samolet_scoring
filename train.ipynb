{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas catboost scikit-learn xgboost lightgbm numpy==1.24.0 tqdm joblib shap optuna flask feature_engine networkx -q\n",
    "# после установки - перезапустите окружение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:48:24.524016Z",
     "start_time": "2024-10-22T10:48:24.490604Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 110884,
     "status": "ok",
     "timestamp": 1723712333124,
     "user": {
      "displayName": "Sonya Kyshildo",
      "userId": "06895829020768772584"
     },
     "user_tz": -420
    },
    "id": "WzP_yNPNPwNy",
    "outputId": "6a769c76-9322-43d7-f73d-aa9f5ce759a6"
   },
   "outputs": [],
   "source": [
    "from lazypredict.Supervised import LazyClassifier\n",
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import train_test_split, GroupKFold, GridSearchCV, ParameterGrid\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, make_scorer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from feature_engine.imputation import MeanMedianImputer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import shap\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Gini</th>\n",
       "      <th>RocAuc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.601087329072902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>0.593574896368103</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.587829107293975</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>0.585739300129877</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BaggingClassifier</td>\n",
       "      <td>0.563008177390122</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.436678197265473</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model              Gini  RocAuc\n",
       "0  RandomForestClassifier 0.601087329072902     NaN\n",
       "1      CatBoostClassifier 0.593574896368103     NaN\n",
       "2           XGBClassifier 0.587829107293975     NaN\n",
       "3          LGBMClassifier 0.585739300129877     NaN\n",
       "4       BaggingClassifier 0.563008177390122     NaN\n",
       "5                     SVM 0.436678197265473     NaN"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.float_format = '{:.15f}'.format\n",
    "models = pd.read_csv('models.csv')\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.RocAuc = models.Gini.apply(lambda x: (x + 1) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Gini</th>\n",
       "      <th>RocAuc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.601087329072902</td>\n",
       "      <td>0.800543664536451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CatBoostClassifier</td>\n",
       "      <td>0.593574896368103</td>\n",
       "      <td>0.796787448184051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.587829107293975</td>\n",
       "      <td>0.793914553646987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>0.585739300129877</td>\n",
       "      <td>0.792869650064939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BaggingClassifier</td>\n",
       "      <td>0.563008177390122</td>\n",
       "      <td>0.781504088695061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.436678197265473</td>\n",
       "      <td>0.718339098632736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model              Gini            RocAuc\n",
       "0  RandomForestClassifier 0.601087329072902 0.800543664536451\n",
       "1      CatBoostClassifier 0.593574896368103 0.796787448184051\n",
       "2           XGBClassifier 0.587829107293975 0.793914553646987\n",
       "3          LGBMClassifier 0.585739300129877 0.792869650064939\n",
       "4       BaggingClassifier 0.563008177390122 0.781504088695061\n",
       "5                     SVM 0.436678197265473 0.718339098632736"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Init data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:48:25.855750Z",
     "start_time": "2024-10-22T10:48:25.075352Z"
    },
    "executionInfo": {
     "elapsed": 8811,
     "status": "ok",
     "timestamp": 1723712442742,
     "user": {
      "displayName": "Sonya Kyshildo",
      "userId": "06895829020768772584"
     },
     "user_tz": -420
    },
    "id": "YF926ladPwN2"
   },
   "outputs": [],
   "source": [
    "X = pd.read_csv('data/train_X.csv')\n",
    "y = pd.read_csv('data/train_y.csv')\n",
    "test = pd.read_csv('data/test2_X.csv')\n",
    "train = X.merge(y, on=['contract_id', 'report_date'])\n",
    "graph = pd.read_csv('data/graph.csv').drop(columns='Unnamed: 0').rename(columns={'contractor_id1': 'contractor_id'})\n",
    "full_graph = graph.pivot_table(index='contractor_id', columns='contractor_id2', values='Distance').fillna(0).reset_index()\n",
    "features_df = pd.read_excel('описание.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:48:25.896040Z",
     "start_time": "2024-10-22T10:48:25.857403Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Делаем аггрегации над дистанциями в графе и группируем по contractor_id,\n",
    "далее будем использовать эти фичи.\n",
    "\"\"\"\n",
    "\n",
    "connection_count = graph.groupby('contractor_id').size().to_dict()\n",
    "mean_distance = graph.groupby('contractor_id').Distance.mean().to_dict()\n",
    "min_distance = graph.groupby('contractor_id').Distance.min().to_dict()\n",
    "max_distance = graph.groupby('contractor_id').Distance.max().to_dict()\n",
    "var_distance = graph.groupby('contractor_id').Distance.var().to_dict()\n",
    "median_distance = graph.groupby('contractor_id').Distance.median().to_dict()\n",
    "std_distance = graph.groupby('contractor_id').Distance.std().to_dict()\n",
    "count_distance = graph.groupby('contractor_id').Distance.count().to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmagVuTZPwOE"
   },
   "source": [
    "**Feature selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:48:25.916800Z",
     "start_time": "2024-10-22T10:48:25.901122Z"
    }
   },
   "outputs": [],
   "source": [
    "class FeatureSelectionModel:\n",
    "    \"\"\"\n",
    "    Class for the selection feature. 4 different models and 6 filters are used to select features.\n",
    "    First, all models are trained to further extract their feature importance.\n",
    "    Next, we extract shap-values ​​from the CatBoostClassifier model and the correlation with the target.\n",
    "    After this, the top 280 features in each filter are searched and the intersections of the lists are selected.\n",
    "    In this way, an average of 70-80 features out of 2100 are selected.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.cb_model = CatBoostClassifier(use_best_model=True, eval_metric='AUC')\n",
    "        self.rf_model = RandomForestClassifier(5000, n_jobs=-1)\n",
    "        self.xgb_model = XGBClassifier(n_estimators=100)\n",
    "        self.lgb_model = LGBMClassifier(n_estimators=100, eval_metric='AUC')\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit Classification algorithms to X and y.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like,\n",
    "            Training vectors, where rows is the number of samples\n",
    "            and columns is the number of features.\n",
    "        y : array-like,\n",
    "            Training vectors, where rows is the number of samples\n",
    "            and columns is the number of features.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y.map(int)\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=0.1,\n",
    "                                                                                random_state=42, stratify=self.y,\n",
    "                                                                                shuffle=True)\n",
    "        self.cb_model.fit(self.X_train, self.y_train, verbose=False, eval_set=(self.X_test, self.y_test))\n",
    "        self.rf_model.fit(self.X_train, self.y_train)\n",
    "        self.xgb_model.fit(self.X, self.y)\n",
    "        self.lgb_model.fit(self.X, self.y)\n",
    "\n",
    "    def get_feature_importance(self):\n",
    "        \"\"\"Get feature importance algorithms to data.\"\"\"\n",
    "        self.t = self.cb_model.get_feature_importance(prettified=True)\n",
    "\n",
    "        self.rf = pd.DataFrame(self.rf_model.feature_importances_.T,\n",
    "                               self.X_train.columns,\n",
    "                               columns=['coef']) \\\n",
    "            .sort_values(by='coef', ascending=False)\n",
    "\n",
    "        self.xgb = pd.DataFrame(list(self.xgb_model.feature_importances_),\n",
    "                                self.X_train.columns,\n",
    "                                columns=['coef']) \\\n",
    "            .sort_values(by='coef', ascending=False)\n",
    "\n",
    "        self.lgb = pd.DataFrame(list(self.lgb_model.feature_importances_),\n",
    "                                self.X_train.columns,\n",
    "                                columns=['coef']) \\\n",
    "            .sort_values(by='coef', ascending=False)\n",
    "\n",
    "        self.explainer = shap.TreeExplainer(self.cb_model)\n",
    "\n",
    "        self.val_dataset = Pool(data=self.X_test, label=self.y_test)\n",
    "        self.shap_values = self.explainer.shap_values(self.val_dataset)\n",
    "        self.mean_abs_shap = np.abs(self.shap_values).mean(axis=0)\n",
    "        self.fi_df = pd.DataFrame({\n",
    "            'feature': self.X_test.columns,\n",
    "            'importance': self.mean_abs_shap\n",
    "        })\n",
    "        self.fi_df = self.fi_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "        self.temp = self.X.copy()\n",
    "        self.temp['default6'] = self.y\n",
    "        self.correlation = self.temp.corrwith(self.temp.default6).abs()\n",
    "\n",
    "    def get_features(self) -> list:\n",
    "        \"\"\"Get features, which were selected.\n",
    "        Returns\n",
    "        -------\n",
    "        features : list\n",
    "            Returns features which were selected in list type.\n",
    "        \"\"\"\n",
    "        self.get_feature_importance()\n",
    "        self.cb_features = list(self.t[self.t.Importances > 0.01]['Feature Id'])[:280]\n",
    "        self.shap_values_features = list(self.fi_df.head(281).feature)\n",
    "        self.rf_features = list(self.rf.index)[:280]\n",
    "        self.xgb_features = list(self.xgb.index)[:280]\n",
    "        self.lgb_features = list(self.lgb.index)[:280]\n",
    "        self.corr_features = list(\n",
    "            self.correlation.sort_values(ascending=False).head(281).drop(columns='default6').index)\n",
    "        self.total = []\n",
    "\n",
    "        for i in self.corr_features:\n",
    "            if i in self.cb_features and i in self.shap_values_features and i in self.rf_features and i in self.lgb_features and i in self.xgb_features:\n",
    "                self.total.append(i)\n",
    "\n",
    "        return self.total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature generation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом блоке кода мы создаем функции даты и финансовые атрибуты. На ставку по умолчанию влияет сезонность,\n",
    "поэтому мы создадим функцию сезона, используя функцию get_ Season. Далее мы переведем функции Contract_date и report_date.\n",
    "в формат даты и времени и извлекайте из них такие данные, как месяц, день, день недели и год. \n",
    "Также с помощью этих возможностей можно сделать функцию Contract_duration — длительность контракта путем вычитания одной даты из другой. \n",
    "Теперь вы можете обратить внимание на таблички с налогами и сделать новую функцию - сумму всех налогов. \n",
    "Также можно округлить атрибут с кредитами до 2 знаков после запятой, чтобы получилось что-то похожее на нормализованный класс — кредиты.\n",
    "Нелишним будет сделать пару финансовых особенностей: превышают ли доходы расходы с налогами и без них."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:48:26.152570Z",
     "start_time": "2024-10-22T10:48:26.141741Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1723712442742,
     "user": {
      "displayName": "Sonya Kyshildo",
      "userId": "06895829020768772584"
     },
     "user_tz": -420
    },
    "id": "5Of3jBBpPwN3"
   },
   "outputs": [],
   "source": [
    "def get_season(date):\n",
    "    month = date.month\n",
    "    if month in [12, 1, 2]:\n",
    "        return 0  # Зима\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 1  # Весна\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 2  # Лето\n",
    "    else:\n",
    "        return 3  # Осень\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция создания дополнительных функций на основе дат, текущий и начальный контракт контракта, а также другие финансовые атрибуты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:48:26.389452Z",
     "start_time": "2024-10-22T10:48:26.378661Z"
    },
    "executionInfo": {
     "elapsed": 887,
     "status": "ok",
     "timestamp": 1723712443628,
     "user": {
      "displayName": "Sonya Kyshildo",
      "userId": "06895829020768772584"
     },
     "user_tz": -420
    },
    "id": "0WEh8gRuPwN4"
   },
   "outputs": [],
   "source": [
    "def base_features(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    data = data.copy()\n",
    "    data['season'] = pd.to_datetime(data['contract_date']).apply(get_season)  # сезон подписания контракта\n",
    "    data['month'] = pd.to_datetime(data['contract_date']).dt.month.apply(int)  # месяц подписания контракта\n",
    "    data['day'] = pd.to_datetime(data['contract_date']).dt.day.apply(int)  # день подписания контракта\n",
    "    data['day_of_week'] = pd.to_datetime(data['contract_date']).dt.dayofweek.apply(\n",
    "        int)  # день недели подписания контракта\n",
    "    data['year'] = pd.to_datetime(data['contract_date']).dt.year.apply(int)  # год подписания контракта\n",
    "    data['report_month'] = pd.to_datetime(data['report_date']).dt.month.apply(int)  # месяц среза\n",
    "    data['report_day'] = pd.to_datetime(data['report_date']).dt.day.apply(int)  # день среза\n",
    "    data['contract_duration'] = (pd.to_datetime(data['report_date']) - pd.to_datetime(\n",
    "        data['contract_date'])).dt.days  # длительность контракта\n",
    "    data['time'] = pd.to_datetime(data['contract_date']).astype(int) / 10 ** 11  # datetime -> timestamp\n",
    "    data['report_time'] = pd.to_datetime(data['report_date']).astype(int) / 10 ** 11  # datetime -> timestamp\n",
    "    data['total_claims_last_12_months'] = data['agg_ArbitrationCases__g_contractor__DefendantSum__sum__12M'] + data[\n",
    "        'agg_ArbitrationCases__g_contractor__PlaintiffSum__sum__12M']  # сумма всех сборов за 12 месяцев\n",
    "    data['total_claims_last_24_months'] = data['agg_ArbitrationCases__g_contractor__DefendantSum__sum__12_24M'] + data[\n",
    "        'agg_ArbitrationCases__g_contractor__PlaintiffSum__sum__12_24M']  # сумма всех сборов за 24 месяца\n",
    "    data['Income > Expenses'] = data['agg_FinanceAndTaxesFTS__g_contractor__Income__last__ALL_TIME'] > data[\n",
    "        'agg_FinanceAndTaxesFTS__g_contractor__Expenses__last__ALL_TIME']  # доходы > расходы?\n",
    "    data['Income > Expenses'] = data['Income > Expenses'].map(int)\n",
    "    data['Income > Taxes + Expenses'] = data['agg_FinanceAndTaxesFTS__g_contractor__Income__last__ALL_TIME'] > (\n",
    "                data['agg_FinanceAndTaxesFTS__g_contractor__TaxesSum__last__ALL_TIME'] + data[\n",
    "            'agg_FinanceAndTaxesFTS__g_contractor__Expenses__last__ALL_TIME'])  # доходы > расходы + налоги?\n",
    "    data['Income > Taxes + Expenses'] = data['Income > Taxes + Expenses'].map(int)\n",
    "    data['credits'] = data['agg_spark_extended_report__g_contractor__CreditLimitSum__last__ALL_TIME'].round(\n",
    "        2)  # округляем признак с кредитами\n",
    "    data['tax'] = data['agg_FinanceAndTaxesFTS__g_contractor__TaxArrearsSum__last__ALL_TIME'] + data[\n",
    "        'agg_FinanceAndTaxesFTS__g_contractor__TaxPenaltiesSum__last__ALL_TIME'] + data[\n",
    "                      'agg_FinanceAndTaxesFTS__g_contractor__TaxesSum__last__ALL_TIME']  # сумма всех налогов\n",
    "    data['contract_current_sum_mean_3M'] = data.groupby('contract_id')['contract_current_sum'].rolling(window=3,\n",
    "                                                                                                       min_periods=1).mean().reset_index(\n",
    "        0, drop=True)  # средняя сумма за 3 месяца по contract_id\n",
    "    data['contract_sum_change_ratio'] = data['contract_current_sum'] / data['contract_init_sum']\n",
    "    data['mean_weekly_abs_price_change'] = data[\n",
    "        [f'agg_all_contracts__g_contract__abs_change_price_last_ds__isMain__last__ALL_TIME',\n",
    "         f'agg_all_contracts__g_contract__abs_change_price_last_ds__isMain__mean__ALL_TIME']].mean(axis=1)\n",
    "    data['arbitration_cases_12M'] = data[f'agg_ArbitrationCases__g_contractor__DefendantSum__sum__12M'] + \\\n",
    "                                    data[f'agg_ArbitrationCases__g_contractor__PlaintiffSum__sum__12M']\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция создания дополнительных признаков на основе вычитания одних признаков из других,\n",
    "а именно показатели за разные периоды времени. Таким образом, мы создаем приращение функции индикатора.\n",
    "Также для каждого показателя создается атрибут среднего значения и изменения по сравнению со средним и исходным показателем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:48:26.731005Z",
     "start_time": "2024-10-22T10:48:26.714310Z"
    }
   },
   "outputs": [],
   "source": [
    "def sums_features(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    data = data.copy()\n",
    "\n",
    "    data['c1'] = data['agg_cec_requests__g_contract__request_id__all__count__2W'] - data[\n",
    "        'agg_cec_requests__g_contract__request_id__all__count__1W']\n",
    "    data['c2'] = data['agg_cec_requests__g_contract__request_id__all__count__3W'] - data[\n",
    "        'agg_cec_requests__g_contract__request_id__all__count__2W']\n",
    "    data['c3'] = data['agg_cec_requests__g_contract__request_id__all__count__4W'] - data[\n",
    "        'agg_cec_requests__g_contract__request_id__all__count__3W']\n",
    "    data['c4'] = data['agg_cec_requests__g_contract__request_id__all__count__5W'] - data[\n",
    "        'agg_cec_requests__g_contract__request_id__all__count__4W']\n",
    "    data['c5'] = data['agg_cec_requests__g_contract__request_id__all__count__6W'] - data[\n",
    "        'agg_cec_requests__g_contract__request_id__all__count__5W']\n",
    "    data['c6'] = data['agg_cec_requests__g_contract__request_id__all__count__7W'] - data[\n",
    "        'agg_cec_requests__g_contract__request_id__all__count__6W']\n",
    "    data['c7'] = data['agg_cec_requests__g_contract__request_id__all__count__8W'] - data[\n",
    "        'agg_cec_requests__g_contract__request_id__all__count__7W']\n",
    "    data['c8'] = data['agg_cec_requests__g_contract__request_id__all__count__12W'] - data[\n",
    "        'agg_cec_requests__g_contract__request_id__all__count__8W']\n",
    "    data['c9'] = data['agg_cec_requests__g_contract__request_id__all__count__ALL_TIME'] - data[\n",
    "        'agg_cec_requests__g_contract__request_id__all__count__12W']\n",
    "    data['c_mean'] = (data['c1'] + data['c2'] + data['c3'] + data['c4'] + data['c5'] + data['c6'] + data['c7'] + data[\n",
    "        'c8'] + data['c9']) / 9\n",
    "    data['c_change'] = data['c_mean'] / data['c1']\n",
    "\n",
    "    data['s1'] = data['agg_cec_requests__g_contract__total_sum_accepted__all__sum__2W'] - data[\n",
    "        'agg_cec_requests__g_contract__total_sum_accepted__all__sum__1W']\n",
    "    data['s2'] = data['agg_cec_requests__g_contract__total_sum_accepted__all__sum__3W'] - data[\n",
    "        'agg_cec_requests__g_contract__total_sum_accepted__all__sum__2W']\n",
    "    data['s3'] = data['agg_cec_requests__g_contract__total_sum_accepted__all__sum__4W'] - data[\n",
    "        'agg_cec_requests__g_contract__total_sum_accepted__all__sum__3W']\n",
    "    data['s4'] = data['agg_cec_requests__g_contract__total_sum_accepted__all__sum__5W'] - data[\n",
    "        'agg_cec_requests__g_contract__total_sum_accepted__all__sum__4W']\n",
    "    data['s5'] = data['agg_cec_requests__g_contract__total_sum_accepted__all__sum__6W'] - data[\n",
    "        'agg_cec_requests__g_contract__total_sum_accepted__all__sum__5W']\n",
    "    data['s6'] = data['agg_cec_requests__g_contract__total_sum_accepted__all__sum__7W'] - data[\n",
    "        'agg_cec_requests__g_contract__total_sum_accepted__all__sum__6W']\n",
    "    data['s7'] = data['agg_cec_requests__g_contract__total_sum_accepted__all__sum__8W'] - data[\n",
    "        'agg_cec_requests__g_contract__total_sum_accepted__all__sum__7W']\n",
    "    data['s8'] = data['agg_cec_requests__g_contract__total_sum_accepted__all__sum__12W'] - data[\n",
    "        'agg_cec_requests__g_contract__total_sum_accepted__all__sum__8W']\n",
    "    data['s9'] = data['agg_cec_requests__g_contract__total_sum_accepted__all__sum__ALL_TIME'] - data[\n",
    "        'agg_cec_requests__g_contract__total_sum_accepted__all__sum__12W']\n",
    "    data['s_mean'] = (data['s1'] + data['s2'] + data['s3'] + data['s4'] + data['s5'] + data['s6'] + data['s7'] + data[\n",
    "        's8'] + data['s9']) / 9\n",
    "    data['s_change'] = data['s_mean'] / data['s1']\n",
    "\n",
    "    data['m1'] = data['agg_cec_requests__g_contract__time_btw_requests__all__mean__2M'] - data[\n",
    "        'agg_cec_requests__g_contract__time_btw_requests__all__mean__1M']\n",
    "    data['m2'] = data['agg_cec_requests__g_contract__time_btw_requests__all__mean__3M'] - data[\n",
    "        'agg_cec_requests__g_contract__time_btw_requests__all__mean__2M']\n",
    "    data['m3'] = data['agg_cec_requests__g_contract__time_btw_requests__all__mean__4M'] - data[\n",
    "        'agg_cec_requests__g_contract__time_btw_requests__all__mean__3M']\n",
    "    data['m4'] = data['agg_cec_requests__g_contract__time_btw_requests__all__mean__5M'] - data[\n",
    "        'agg_cec_requests__g_contract__time_btw_requests__all__mean__4M']\n",
    "    data['m5'] = data['agg_cec_requests__g_contract__time_btw_requests__all__mean__6M'] - data[\n",
    "        'agg_cec_requests__g_contract__time_btw_requests__all__mean__5M']\n",
    "    data['m6'] = data['agg_cec_requests__g_contract__time_btw_requests__all__mean__7M'] - data[\n",
    "        'agg_cec_requests__g_contract__time_btw_requests__all__mean__6M']\n",
    "    data['m7'] = data['agg_cec_requests__g_contract__time_btw_requests__all__mean__8M'] - data[\n",
    "        'agg_cec_requests__g_contract__time_btw_requests__all__mean__7M']\n",
    "    data['m8'] = data['agg_cec_requests__g_contract__time_btw_requests__all__mean__12M'] - data[\n",
    "        'agg_cec_requests__g_contract__time_btw_requests__all__mean__8M']\n",
    "    data['m9'] = data['agg_cec_requests__g_contract__time_btw_requests__all__mean__ALL_TIME'] - data[\n",
    "        'agg_cec_requests__g_contract__time_btw_requests__all__mean__12M']\n",
    "    data['m_mean'] = (data['m1'] + data['m2'] + data['m3'] + data['m4'] + data['m5'] + data['m6'] + data['m7'] + data[\n",
    "        'm8'] + data['m9']) / 9\n",
    "    data['m_change'] = data['m_mean'] / data['m1']\n",
    "\n",
    "    data['d1'] = data['agg_payments__g_contract__sum__all__countDistinct__2W'] - data[\n",
    "        'agg_payments__g_contract__sum__all__countDistinct__1W']\n",
    "    data['d2'] = data['agg_payments__g_contract__sum__all__countDistinct__4W'] - data[\n",
    "        'agg_payments__g_contract__sum__all__countDistinct__2W']\n",
    "    data['d3'] = data['agg_payments__g_contract__sum__all__countDistinct__8W'] - data[\n",
    "        'agg_payments__g_contract__sum__all__countDistinct__4W']\n",
    "    data['d4'] = data['agg_payments__g_contract__sum__all__countDistinct__12W'] - data[\n",
    "        'agg_payments__g_contract__sum__all__countDistinct__8W']\n",
    "    data['d5'] = data['agg_payments__g_contract__sum__all__countDistinct__ALL_TIME'] - data[\n",
    "        'agg_payments__g_contract__sum__all__countDistinct__12W']\n",
    "    data['d_mean'] = (data['d1'] + data['d2'] + data['d3'] + data['d4'] + data['d5']) / 5\n",
    "    data['d_change'] = data['d_mean'] / data['d1']\n",
    "\n",
    "    data['a1'] = data['agg_payments__g_contract__sum__all__sum__2W'] - data[\n",
    "        'agg_payments__g_contract__sum__all__sum__1W']\n",
    "    data['a2'] = data['agg_payments__g_contract__sum__all__sum__4W'] - data[\n",
    "        'agg_payments__g_contract__sum__all__sum__2W']\n",
    "    data['a3'] = data['agg_payments__g_contract__sum__all__sum__8W'] - data[\n",
    "        'agg_payments__g_contract__sum__all__sum__4W']\n",
    "    data['a4'] = data['agg_payments__g_contract__sum__all__sum__12W'] - data[\n",
    "        'agg_payments__g_contract__sum__all__sum__8W']\n",
    "    data['a5'] = data['agg_payments__g_contract__sum__all__sum__ALL_TIME'] - data[\n",
    "        'agg_payments__g_contract__sum__all__sum__12W']\n",
    "    data['a_mean'] = (data['a1'] + data['a2'] + data['a3'] + data['a4'] + data['a5']) / 5\n",
    "    data['a_change'] = data['a_mean'] / data['a1']\n",
    "\n",
    "    data['ac1'] = data['agg_ks2__g_contract__id__all__count__2W'] - data['agg_ks2__g_contract__id__all__count__1W']\n",
    "    data['ac2'] = data['agg_ks2__g_contract__id__all__count__4W'] - data['agg_ks2__g_contract__id__all__count__2W']\n",
    "    data['ac3'] = data['agg_ks2__g_contract__id__all__count__8W'] - data['agg_ks2__g_contract__id__all__count__4W']\n",
    "    data['ac4'] = data['agg_ks2__g_contract__id__all__count__12W'] - data['agg_ks2__g_contract__id__all__count__8W']\n",
    "    data['ac5'] = data['agg_ks2__g_contract__id__all__count__ALL_TIME'] - data[\n",
    "        'agg_ks2__g_contract__id__all__count__12W']\n",
    "    data['ac_mean'] = (data['ac1'] + data['ac2'] + data['ac3'] + data['ac4'] + data['ac5']) / 5\n",
    "    data['ac_change'] = data['ac_mean'] / data['ac1']\n",
    "\n",
    "    data['as1'] = data['agg_ks2__g_contract__total_sum__all__sum__2W'] - data[\n",
    "        'agg_ks2__g_contract__total_sum__all__sum__1W']\n",
    "    data['as2'] = data['agg_ks2__g_contract__total_sum__all__sum__4W'] - data[\n",
    "        'agg_ks2__g_contract__total_sum__all__sum__2W']\n",
    "    data['as3'] = data['agg_ks2__g_contract__total_sum__all__sum__8W'] - data[\n",
    "        'agg_ks2__g_contract__total_sum__all__sum__4W']\n",
    "    data['as4'] = data['agg_ks2__g_contract__total_sum__all__sum__12W'] - data[\n",
    "        'agg_ks2__g_contract__total_sum__all__sum__8W']\n",
    "    data['as5'] = data['agg_ks2__g_contract__total_sum__all__sum__ALL_TIME'] - data[\n",
    "        'agg_ks2__g_contract__total_sum__all__sum__12W']\n",
    "    data['as_mean'] = (data['as1'] + data['as2'] + data['as3'] + data['as4'] + data['as5']) / 5\n",
    "    data['as_change'] = data['as_mean'] / data['as1']\n",
    "\n",
    "    data['w1'] = data['agg_spass_applications__g_contract__appl_count_week__mean__2W'] - data[\n",
    "        'agg_spass_applications__g_contract__appl_count_week__mean__1W']\n",
    "    data['w2'] = data['agg_spass_applications__g_contract__appl_count_week__mean__3W'] - data[\n",
    "        'agg_spass_applications__g_contract__appl_count_week__mean__2W']\n",
    "    data['w3'] = data['agg_spass_applications__g_contract__appl_count_week__mean__4W'] - data[\n",
    "        'agg_spass_applications__g_contract__appl_count_week__mean__3W']\n",
    "    data['w4'] = data['agg_spass_applications__g_contract__appl_count_week__mean__5W'] - data[\n",
    "        'agg_spass_applications__g_contract__appl_count_week__mean__4W']\n",
    "    data['w5'] = data['agg_spass_applications__g_contract__appl_count_week__mean__6W'] - data[\n",
    "        'agg_spass_applications__g_contract__appl_count_week__mean__5W']\n",
    "    data['w6'] = data['agg_spass_applications__g_contract__appl_count_week__mean__8W'] - data[\n",
    "        'agg_spass_applications__g_contract__appl_count_week__mean__6W']\n",
    "    data['w7'] = data['agg_spass_applications__g_contract__appl_count_week__mean__12W'] - data[\n",
    "        'agg_spass_applications__g_contract__appl_count_week__mean__8W']\n",
    "    data['w8'] = data['agg_spass_applications__g_contract__appl_count_week__mean__26W'] - data[\n",
    "        'agg_spass_applications__g_contract__appl_count_week__mean__12W']\n",
    "    data['w9'] = data['agg_spass_applications__g_contract__appl_count_week__mean__ALL_TIME'] - data[\n",
    "        'agg_spass_applications__g_contract__appl_count_week__mean__26W']\n",
    "    data['w_mean'] = (data['w1'] + data['w2'] + data['w3'] + data['w4'] + data['w5'] + data['w6'] + data['w7'] + data[\n",
    "        'w8'] + data['w9']) / 9\n",
    "    data['w_change'] = data['w_mean'] / data['w1']\n",
    "\n",
    "    data['f1'] = data['agg_workers__g_contract__fact_workers__all__mean__2W'] - data[\n",
    "        'agg_workers__g_contract__fact_workers__all__mean__1W']\n",
    "    data['f2'] = data['agg_workers__g_contract__fact_workers__all__mean__3W'] - data[\n",
    "        'agg_workers__g_contract__fact_workers__all__mean__2W']\n",
    "    data['f3'] = data['agg_workers__g_contract__fact_workers__all__mean__4W'] - data[\n",
    "        'agg_workers__g_contract__fact_workers__all__mean__3W']\n",
    "    data['f4'] = data['agg_workers__g_contract__fact_workers__all__mean__5W'] - data[\n",
    "        'agg_workers__g_contract__fact_workers__all__mean__4W']\n",
    "    data['f5'] = data['agg_workers__g_contract__fact_workers__all__mean__6W'] - data[\n",
    "        'agg_workers__g_contract__fact_workers__all__mean__5W']\n",
    "    data['f6'] = data['agg_workers__g_contract__fact_workers__all__mean__8W'] - data[\n",
    "        'agg_workers__g_contract__fact_workers__all__mean__6W']\n",
    "    data['f7'] = data['agg_workers__g_contract__fact_workers__all__mean__12W'] - data[\n",
    "        'agg_workers__g_contract__fact_workers__all__mean__8W']\n",
    "    data['f8'] = data['agg_workers__g_contract__fact_workers__all__mean__26W'] - data[\n",
    "        'agg_workers__g_contract__fact_workers__all__mean__12W']\n",
    "    data['f9'] = data['agg_workers__g_contract__fact_workers__all__mean__ALL_TIME'] - data[\n",
    "        'agg_workers__g_contract__fact_workers__all__mean__26W']\n",
    "    data['f_mean'] = (data['f1'] + data['f2'] + data['f3'] + data['f4'] + data['f5'] + data['f6'] + data['f7'] + data[\n",
    "        'f8'] + data['f9']) / 9\n",
    "    data['f_change'] = data['f_mean'] / data['f1']\n",
    "\n",
    "    data['o1'] = data['agg_materials__g_contract__order_id__countDistinct__2W'] - data[\n",
    "        'agg_materials__g_contract__order_id__countDistinct__1W']\n",
    "    data['o2'] = data['agg_materials__g_contract__order_id__countDistinct__4W'] - data[\n",
    "        'agg_materials__g_contract__order_id__countDistinct__2W']\n",
    "    data['o3'] = data['agg_materials__g_contract__order_id__countDistinct__8W'] - data[\n",
    "        'agg_materials__g_contract__order_id__countDistinct__4W']\n",
    "    data['o4'] = data['agg_materials__g_contract__order_id__countDistinct__12W'] - data[\n",
    "        'agg_materials__g_contract__order_id__countDistinct__8W']\n",
    "    data['o5'] = data['agg_materials__g_contract__order_id__countDistinct__ALL_TIME'] - data[\n",
    "        'agg_materials__g_contract__order_id__countDistinct__12W']\n",
    "    data['o_mean'] = (data['o1'] + data['o2'] + data['o3'] + data['o4'] + data['o5']) / 5\n",
    "    data['o_change'] = data['o_mean'] / data['o1']\n",
    "\n",
    "    data['i1'] = data['agg_sroomer__g_contractor__sroomer_id__count__6M'] - data[\n",
    "        'agg_sroomer__g_contractor__sroomer_id__count__3M']\n",
    "    data['i2'] = data['agg_sroomer__g_contractor__sroomer_id__count__12M'] - data[\n",
    "        'agg_sroomer__g_contractor__sroomer_id__count__6M']\n",
    "    data['i3'] = data['agg_sroomer__g_contractor__sroomer_id__count__ALL_TIME'] - data[\n",
    "        'agg_sroomer__g_contractor__sroomer_id__count__12M']\n",
    "    data['i_mean'] = (data['i1'] + data['i2'] + data['i3']) / 3\n",
    "    data['i_change'] = data['i_mean'] / data['i1']\n",
    "\n",
    "    data['ds1'] = data['agg_ArbitrationCases__g_contractor__DefendantSum__sum__12_24M'] - data[\n",
    "        'agg_ArbitrationCases__g_contractor__DefendantSum__sum__12M']\n",
    "    data['ds2'] = data['agg_ArbitrationCases__g_contractor__DefendantSum__sum__12_36M'] - data[\n",
    "        'agg_ArbitrationCases__g_contractor__DefendantSum__sum__12_24M']\n",
    "    data['ds3'] = data['agg_ArbitrationCases__g_contractor__DefendantSum__sum__12_48M'] - data[\n",
    "        'agg_ArbitrationCases__g_contractor__DefendantSum__sum__12_36M']\n",
    "    data['ds4'] = data['agg_ArbitrationCases__g_contractor__DefendantSum__sum__ALL_TIME'] - data[\n",
    "        'agg_ArbitrationCases__g_contractor__DefendantSum__sum__12_48M']\n",
    "    data['ds_mean'] = (data['ds1'] + data['ds2'] + data['ds3'] + data['ds4']) / 4\n",
    "    data['ds_change'] = data['ds_mean'] / data['ds1']\n",
    "\n",
    "    data['p1'] = data['agg_ArbitrationCases__g_contractor__PlaintiffSum__sum__12_24M'] - data[\n",
    "        'agg_ArbitrationCases__g_contractor__PlaintiffSum__sum__12M']\n",
    "    data['p2'] = data['agg_ArbitrationCases__g_contractor__PlaintiffSum__sum__12_36M'] - data[\n",
    "        'agg_ArbitrationCases__g_contractor__PlaintiffSum__sum__12_24M']\n",
    "    data['p3'] = data['agg_ArbitrationCases__g_contractor__PlaintiffSum__sum__12_48M'] - data[\n",
    "        'agg_ArbitrationCases__g_contractor__PlaintiffSum__sum__12_36M']\n",
    "    data['p4'] = data['agg_ArbitrationCases__g_contractor__PlaintiffSum__sum__ALL_TIME'] - data[\n",
    "        'agg_ArbitrationCases__g_contractor__PlaintiffSum__sum__12_48M']\n",
    "    data['p_mean'] = (data['p1'] + data['p2'] + data['p3'] + data['p4']) / 4\n",
    "    data['p_change'] = data['p_mean'] / data['p1']\n",
    "\n",
    "    data['cd1'] = data['agg_tender_proposal__g_contractor__id__ALL__countDistinct__2W'] - data[\n",
    "        'agg_tender_proposal__g_contractor__id__ALL__countDistinct__1W']\n",
    "    data['cd2'] = data['agg_tender_proposal__g_contractor__id__ALL__countDistinct__4W'] - data[\n",
    "        'agg_tender_proposal__g_contractor__id__ALL__countDistinct__2W']\n",
    "    data['cd3'] = data['agg_tender_proposal__g_contractor__id__ALL__countDistinct__8W'] - data[\n",
    "        'agg_tender_proposal__g_contractor__id__ALL__countDistinct__4W']\n",
    "    data['cd4'] = data['agg_tender_proposal__g_contractor__id__ALL__countDistinct__12W'] - data[\n",
    "        'agg_tender_proposal__g_contractor__id__ALL__countDistinct__8W']\n",
    "    data['cd5'] = data['agg_tender_proposal__g_contractor__id__ALL__countDistinct__26W'] - data[\n",
    "        'agg_tender_proposal__g_contractor__id__ALL__countDistinct__12W']\n",
    "    data['cd6'] = data['agg_tender_proposal__g_contractor__id__ALL__countDistinct__52W'] - data[\n",
    "        'agg_tender_proposal__g_contractor__id__ALL__countDistinct__26W']\n",
    "    data['cd7'] = data['agg_tender_proposal__g_contractor__id__ALL__countDistinct__ALL_TIME'] - data[\n",
    "        'agg_tender_proposal__g_contractor__id__ALL__countDistinct__52W']\n",
    "    data['cd_mean'] = (data['cd1'] + data['cd2'] + data['cd3'] + data['cd4'] + data['cd5'] + data['cd6'] + data[\n",
    "        'cd7']) / 7\n",
    "    data['cd_change'] = data['cd_mean'] / data['cd1']\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple model(without data preprocessing)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала, обучим обычную модель на дефолтных данных, без особой предобработки(только с заполнением пропусков),\n",
    "удалим фичи, в которых много NaN и удалим дубликаты строчек, так как в 99% случаев показатель дефолта не меняется.\n",
    "Об этом подробнее есть в ноутбуке EDA.ipynb, там наглядно видно, что подрядчик не меняет свой дефолт практически\n",
    "в 100% случаев. Возьмем эту модель дальше для ансамбля."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "agg_FinanceAndTaxesFTS__g_contractor__TaxPenaltiesSum__last__ALL_TIME              25425\n",
       "agg_all_contracts__g_contract__rel_change_price_last_ds__isMain__mean__ALL_TIME    23736\n",
       "agg_all_contracts__g_contract__rel_change_price_last_ds__isMain__last__ALL_TIME    23736\n",
       "agg_all_contracts__g_contract__abs_change_price_last_ds__isMain__mean__ALL_TIME    21507\n",
       "agg_all_contracts__g_contract__abs_change_price_last_ds__isMain__last__ALL_TIME    21507\n",
       "agg_FinanceAndTaxesFTS__g_contractor__TaxArrearsSum__last__ALL_TIME                18993\n",
       "agg_sroomer__g_contractor__sroomer_id__count__ALL_TIME                             18377\n",
       "agg_sroomer__g_contractor__sroomer_id__count__6M                                   18377\n",
       "agg_sroomer__g_contractor__sroomer_id__count__3M                                   18377\n",
       "agg_sroomer__g_contractor__sroomer_id__count__12M                                  18377\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().sum().sort_values()[::-1].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть некоторые функции, которые содержат много NaN, разумно будет не использовать их и отказаться:\n",
    "agg_all_contracts__g_contract__abs_change_price_last_ds__isMain__last__ALL_TIME,\n",
    "agg_all_contracts__g_contract__abs_change_price_last_ds__isMain__mean__ALL_TIME,\n",
    "agg_all_contracts__g_contract__rel_change_price_last_ds__isMain__last__ALL_TIME,\n",
    "agg_all_contracts__g_contract__rel_change_price_last_ds__isMain__mean__ALL_TIME,\n",
    "agg_FinanceAndTaxesFTS__g_contractor__TaxPenaltiesSum__last__ALL_TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['agg_all_contracts__g_contract__abs_change_price_last_ds__isMain__last__ALL_TIME',\n",
    "            'agg_all_contracts__g_contract__abs_change_price_last_ds__isMain__mean__ALL_TIME',\n",
    "            'agg_all_contracts__g_contract__rel_change_price_last_ds__isMain__last__ALL_TIME',\n",
    "            'agg_all_contracts__g_contract__rel_change_price_last_ds__isMain__mean__ALL_TIME',\n",
    "            'agg_FinanceAndTaxesFTS__g_contractor__TaxPenaltiesSum__last__ALL_TIME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_train = train.copy()\n",
    "copy_train['default6'] = y.default6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "for i in copy_train.isnull().sum().items():\n",
    "    if i[-1] > len(copy_train) * 0.7:\n",
    "        to_drop.append(i[0])\n",
    "    if 0 < i[-1] <= len(copy_train) * 0.7:       \n",
    "        features.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Fill NaN by median\"\"\"\n",
    "copy_train = copy_train.drop(columns=to_drop)\n",
    "imputer = MeanMedianImputer(imputation_method='median', variables=features)\n",
    "imputer.fit(copy_train[features])\n",
    "copy_train[features] = imputer.transform(copy_train[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_train = copy_train.drop(columns=['report_date', 'contract_date'])\n",
    "copy_train = copy_train.drop_duplicates(subset=['contract_id', 'contractor_id', 'default6'])\n",
    "copy_train = copy_train.groupby(['contract_id', 'contractor_id', 'specialization_id', 'project_id'], as_index=False).mean()\n",
    "copy_train = copy_train.drop(columns=['contract_id','project_id', 'building_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_test1 = test.copy()\n",
    "copy_test1 = copy_test1.drop(columns=['contract_id','project_id', 'building_id'])\n",
    "copy_test1[features] = imputer.transform(copy_test1[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = {}\n",
    "contractors2 = list(set(copy_train.contractor_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in graph.iterrows():\n",
    "    if row.contractor_id not in dd:\n",
    "        dd[row.contractor_id] = []\n",
    "    if row.contractor_id2 in contractors2:\n",
    "        dd[row.contractor_id].append((row.contractor_id2, row.Distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in dd:\n",
    "    dd[j] = [i[0] for i in sorted(dd[j], key=lambda x: x[1])][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_simple = copy_train.drop(columns=['default6', 'contractor_id'])\n",
    "y_simple = copy_train['default6'].map(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model = RandomForestClassifier(class_weight='balanced', max_depth=20,\n",
    "                       min_samples_leaf=2, n_estimators=500, random_state=42)\n",
    "simple_model.fit(X_simple, y_simple);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final/simple_model.joblib']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(simple_model, 'final/simple_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В итоге, у нас готова первая модель для ансамбля. Дальше попробуем сделать модели, обученные\n",
    "на данных с хорошей предобработкой и генерацией дополнительных фичей. Так как это самая слабая модель,\n",
    "разумнее всего будет ей поставить небольшой коэфициента для блендинга предсказаний."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data preprocessing for best models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим функции к данным для генерации дополнительных признаков, так как доп. признаки - это\n",
    "всегда хорошо, у модели будет больше вероятность дать правильный ответ.\n",
    "Обрабатываем пропуски средним, делаем one-hot кодирование признаков с ID идентификаторами.\n",
    "Таким образом после фича селекшена модель отберет несколько айдишников, по которым можно\n",
    "довольно точно определить дефолт. У этого способа есть минус - долгое обучение модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:48:27.183653Z",
     "start_time": "2024-10-22T10:48:26.970478Z"
    }
   },
   "outputs": [],
   "source": [
    "train = base_features(train)\n",
    "test = base_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:48:27.396175Z",
     "start_time": "2024-10-22T10:48:27.307110Z"
    }
   },
   "outputs": [],
   "source": [
    "train = sums_features(train)\n",
    "test = sums_features(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение Contract_id для перекрестной проверки\n",
    "Заполняем пропуски в среднем столбце. Затем преобразуем id обратно в int, так как SimpleImputer преобразует все в число с плавающей запятой.\n",
    "Мы выполняем горячее кодирование для функций с идентификаторами, чтобы модель обращала больше внимания на определенные значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:48:28.282838Z",
     "start_time": "2024-10-22T10:48:28.271424Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1723712444567,
     "user": {
      "displayName": "Sonya Kyshildo",
      "userId": "06895829020768772584"
     },
     "user_tz": -420
    },
    "id": "Xurysmn2PwOD"
   },
   "outputs": [],
   "source": [
    "ctrs = train.contract_id\n",
    "contractors1 = train.contractor_id\n",
    "contractors2 = test.contractor_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:48:28.882493Z",
     "start_time": "2024-10-22T10:48:28.809470Z"
    },
    "executionInfo": {
     "elapsed": 349,
     "status": "ok",
     "timestamp": 1723712444911,
     "user": {
      "displayName": "Sonya Kyshildo",
      "userId": "06895829020768772584"
     },
     "user_tz": -420
    },
    "id": "v6jX1DiAPwOD"
   },
   "outputs": [],
   "source": [
    "train = train.drop(columns=['report_date', 'contract_date', 'contract_id'])  # delete dates и contract_id\n",
    "full_df = pd.concat(\n",
    "    [train.drop(columns='default6'), test.drop(columns=['report_date', 'contract_date', 'contract_id'])],\n",
    "    ignore_index=True) # concating train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:48:29.196745Z",
     "start_time": "2024-10-22T10:48:29.155644Z"
    }
   },
   "outputs": [],
   "source": [
    "# id -> int type\n",
    "full_df['specialization_id'] = full_df['specialization_id'].map(int)\n",
    "full_df['project_id'] = full_df['project_id'].map(int)\n",
    "full_df['building_id'] = full_df['building_id'].map(int)\n",
    "full_df['contractor_id'] = full_df['contractor_id'].map(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:48:48.252662Z",
     "start_time": "2024-10-22T10:48:29.466442Z"
    }
   },
   "outputs": [],
   "source": [
    "# one-hot encoding for ID features\n",
    "specs = pd.get_dummies(full_df.specialization_id).map(int).rename(columns=dict(\n",
    "    zip(full_df.specialization_id.unique().tolist(),\n",
    "        [f'spec_{i}' for i in full_df.specialization_id.unique().tolist()])))\n",
    "projects = pd.get_dummies(full_df.project_id).map(int).rename(\n",
    "    columns=dict(zip(full_df.project_id.unique(), [f'project_{i}' for i in full_df.project_id.unique().tolist()])))\n",
    "buildings = pd.get_dummies(full_df.building_id).map(int).rename(columns=dict(\n",
    "    zip(full_df.building_id.unique().tolist(), [f'building_{i}' for i in full_df.building_id.unique().tolist()])))\n",
    "contractors = pd.get_dummies(full_df.contractor_id).map(int).rename(columns=dict(\n",
    "    zip(full_df.contractor_id.unique().tolist(), [f'contractor_{i}' for i in full_df.contractor_id.unique().tolist()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:48:49.650223Z",
     "start_time": "2024-10-22T10:48:48.212556Z"
    }
   },
   "outputs": [],
   "source": [
    "# join ids\n",
    "full_df = full_df.join(specs)\n",
    "full_df = full_df.join(projects)\n",
    "full_df = full_df.join(buildings)\n",
    "full_df = full_df.join(contractors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:48:50.865589Z",
     "start_time": "2024-10-22T10:48:49.613956Z"
    }
   },
   "outputs": [],
   "source": [
    "full_df1 = full_df.copy()\n",
    "full_df2 = full_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data preprocessing for default6 percentage on R distance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее проверим гипотезу - создание признака с процентом дефолта в радиусе R.\n",
    "При помощи графа отберем для каждого подрядчика тех, что находятся в радиусе 20,\n",
    "посчитаем общую сумму их дефолтов и поделим на количество дефолтов. Таким образом,\n",
    "у нас появляется новый признак, который далее будет для модели самым значимым. Это голд-фича\n",
    "нашего решения, ведь это дало нам самый большой бустинг скора на валидации и на лидерборде(примерно на 0.01).\n",
    "Также добавим пару аггрегационных фич, которые сделали в самом начале ноутбука. Это дает неплохой прирост\n",
    "для скора. Самыми лучшими из них будут минимальное расстояние до подрядчиков. Это помогает модели\n",
    "понять, насколько далеко от остальных находится подрядчик. Например, если у него минимальное расстояние будет\n",
    "довольно большое, логично предположить, что с ним плохая идет работа, так как он ни с кем не сотрудничает.\n",
    "Чтобы понимать примерно какие расстояния между подрядчика, добавим признак медианное расстояние, а также его\n",
    "дисперсия. Это также помогает понять модели, какие в целом будут расстояния. Если медиана небольшая, то\n",
    "вероятнее всего этот подрядчик много с кем сотрудничает. А если дисперсия большая, то модель может понять,\n",
    "что скорее всего в данных есть выбросы. Например, в среднем он сотрудничает с подрядчиками на расстоянии до 30,\n",
    "а есть связь с несколькими подрядчиками на расстоянии 30000, что делает дисперсию довольно большой.\n",
    "Точно также заполняем пропуски средним значением."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-22T10:48:19.004271Z"
    }
   },
   "outputs": [],
   "source": [
    "def default_percentage_radius(r: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Функция для поиска процентов по умолчанию 6 в радиусе R.\n",
    "    Для каждого подрядчика выбираются другие подрядчики на расстоянии до R включительно.\n",
    "    Далее вычисляется процент default6 путем деления суммы всех default6 на их количество.\n",
    "    \"\"\"\n",
    "    default6_proc = []\n",
    "\n",
    "    for contractor_id in tqdm(list(full_df.contractor_id.unique())):\n",
    "        l = list(graph[(graph.Distance <= r) & (graph.contractor_id == contractor_id)].contractor_id2.unique())\n",
    "        cnt = 0\n",
    "        default6 = 0\n",
    "\n",
    "        for _, row in train.iterrows():\n",
    "            if row.contractor_id in l or row.contractor_id == contractor_id:\n",
    "                cnt += 1\n",
    "                default6 += row.default6\n",
    "\n",
    "        if cnt != 0:\n",
    "            default6_proc.append({\n",
    "                \"contractor_id\": contractor_id,\n",
    "                \"proc_default6\": default6 / cnt\n",
    "            })\n",
    "        else:\n",
    "            default6_proc.append({\n",
    "                \"contractor_id\": contractor_id,\n",
    "                \"proc_default6\": 0\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(default6_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:48:50.880776Z",
     "start_time": "2024-10-22T10:48:50.865779Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 847/847 [07:55<00:00,  1.78it/s]\n"
     ]
    }
   ],
   "source": [
    "default6_proc = default_percentage_radius(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:48:51.325125Z",
     "start_time": "2024-10-22T10:48:50.874930Z"
    }
   },
   "outputs": [],
   "source": [
    "# агрегации над расстояниями\n",
    "\n",
    "def distance_aggregations_features(df: pd.DataFrame, graph: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Функция для создания признаков агрегирования на основе расстояний на графике и данных df. Возвращает pd.DataFrame с функциями\"\"\"\n",
    "    data = []\n",
    "\n",
    "    for i in list(df.contractor_id.unique()):\n",
    "        try:\n",
    "            a = graph[(graph.contractor_id == i) | (graph.contractor_id2 == i)].sort_values(by='Distance').head(5)\n",
    "            l = []\n",
    "            for _, row in a.iterrows():\n",
    "                if row.contractor_id == i:\n",
    "                    l.append(row.contractor_id2)\n",
    "                else:\n",
    "                    l.append(row.contractor_id)\n",
    "\n",
    "            data.append({\n",
    "                \"contractor_id\": i,\n",
    "                \"min_distance\": min_distance[i],\n",
    "                \"max_distance\": max_distance[i],\n",
    "                \"var_distance\": var_distance[i],\n",
    "                \"median_distance\": median_distance[i],\n",
    "            })\n",
    "        except:\n",
    "            data.append({\n",
    "                \"contractor_id\": i,\n",
    "                \"min_distance\": 0,\n",
    "                \"max_distance\": 0,\n",
    "                \"var_distance\": 0,\n",
    "                \"median_distance\": 0,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = distance_aggregations_features(full_df, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:48:51.735618Z",
     "start_time": "2024-10-22T10:48:51.326490Z"
    }
   },
   "outputs": [],
   "source": [
    "full_df = full_df.merge(data, on='contractor_id') # merge full_df and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:48:52.443773Z",
     "start_time": "2024-10-22T10:48:51.736365Z"
    }
   },
   "outputs": [],
   "source": [
    "full_df = full_df.merge(default6_proc, on='contractor_id') # merge default6_proc and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:48:54.110972Z",
     "start_time": "2024-10-22T10:48:52.428214Z"
    },
    "executionInfo": {
     "elapsed": 720,
     "status": "ok",
     "timestamp": 1723712445630,
     "user": {
      "displayName": "Sonya Kyshildo",
      "userId": "06895829020768772584"
     },
     "user_tz": -420
    },
    "id": "ODITMxOiPwOD"
   },
   "outputs": [],
   "source": [
    "# fill NaN by mean values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df_imputed = imputer.fit_transform(full_df)\n",
    "df_imputed = pd.DataFrame(df_imputed, columns=full_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:48:54.210122Z",
     "start_time": "2024-10-22T10:48:54.064067Z"
    },
    "executionInfo": {
     "elapsed": 424,
     "status": "ok",
     "timestamp": 1723712473519,
     "user": {
      "displayName": "Sonya Kyshildo",
      "userId": "06895829020768772584"
     },
     "user_tz": -420
    },
    "id": "Lich6iOiPwOD"
   },
   "outputs": [],
   "source": [
    "df_imputed = df_imputed.drop(columns=['specialization_id', 'project_id', 'building_id', 'contractor_id'])  # drop id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:48:54.223236Z",
     "start_time": "2024-10-22T10:48:54.211334Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1723713564875,
     "user": {
      "displayName": "Sonya Kyshildo",
      "userId": "06895829020768772584"
     },
     "user_tz": -420
    },
    "id": "-BrkhtDKPwOD"
   },
   "outputs": [],
   "source": [
    "X = df_imputed[:len(train)]  # slice by train\n",
    "y = train.default6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:48:54.225362Z",
     "start_time": "2024-10-22T10:48:54.217461Z"
    }
   },
   "outputs": [],
   "source": [
    "fsmodel = FeatureSelectionModel() # model for Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:50:47.037265Z",
     "start_time": "2024-10-22T10:48:54.220972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Info] Number of positive: 4704, number of negative: 24127\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017188 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 55300\n",
      "[LightGBM] [Info] Number of data points in the train set: 28831, number of used features: 949\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.163158 -> initscore=-1.634918\n",
      "[LightGBM] [Info] Start training from score -1.634918\n"
     ]
    }
   ],
   "source": [
    "fsmodel.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:50:48.964944Z",
     "start_time": "2024-10-22T10:50:47.039324Z"
    }
   },
   "outputs": [],
   "source": [
    "total = fsmodel.get_features() # a list of features to fit final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:50:48.970277Z",
     "start_time": "2024-10-22T10:50:48.965100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total) # quantity of selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2087</th>\n",
       "      <td>proc_default6</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>contract_current_sum_mean_3M</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>contract_current_sum</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>agg_spark_extended_report__g_contractor__Credi...</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>report_time</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>building_529</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>building_527</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>building_525</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>building_524</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>building_711</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2088 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                feature  importance\n",
       "2087                                      proc_default6        0.42\n",
       "178                        contract_current_sum_mean_3M        0.24\n",
       "1                                  contract_current_sum        0.19\n",
       "129   agg_spark_extended_report__g_contractor__Credi...        0.17\n",
       "171                                         report_time        0.12\n",
       "...                                                 ...         ...\n",
       "873                                        building_529        0.00\n",
       "871                                        building_527        0.00\n",
       "869                                        building_525        0.00\n",
       "868                                        building_524        0.00\n",
       "1044                                       building_711        0.00\n",
       "\n",
       "[2088 rows x 2 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fsmodel.fi_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и ожидалось, этот признак является наиболее значимым для модели. Поэтому дальше можно попробовать поработать\n",
    "с соседями подрядчика, так как выяснилось, что показатель дефолта его соседей сильно связаны с дефолтом самого подрядчика.\n",
    "Скорее всего(наше предположение), что если подрядчик сотрудничает с хорошими людьми, выполняющими свою работу, то и\n",
    "он сам ее выполнит, а если с плохими, невыполняющими свои обязательства, аналогично не выполнит заказ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preprocessing top 5 neighbours**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выдвинем гипотезу, что показатель дефолта подрядчика сильно зависит от показателя дефолта его ближайших соседей.\n",
    "Как мы выяснили в прошлом блоке, это оказалось правдой. Можно попробовать провести некоторые\n",
    "операции над соседями подрядчиков. По таблицам с feature importance, мы увидели, что наиболее значимыми\n",
    "являются признаки contract_init_sum и contract_current_sum. Почему бы не отобрать 5 ближайших подрядчиков\n",
    "и использовать их признаки. Но так как в 1 строчку нам нужно подставить значение сразу за все строчки подрядчика,\n",
    "лучше всего будет усреднять эти показатели по контрактору и записывать их в датафрейм. Также, применим аггрегационные фичи,\n",
    "которые делали в прошлом шаге. После обучения, включим эту модель в ансамбль."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:50:49.456874Z",
     "start_time": "2024-10-22T10:50:48.971593Z"
    }
   },
   "outputs": [],
   "source": [
    "def select_top5_neighbours(df: pd.DataFrame, graph: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Функция выбора топ-5 ближайших контрагентов. По каждому подрядчику\n",
    "    5 ближайших из них выбираются и сохраняются в кадре данных.\n",
    "    Если контрагентов нет, во все столбцы добавляются ноли.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    for i in list(df.contractor_id.unique()):\n",
    "        try:\n",
    "            a = graph[(graph.contractor_id == i) | (graph.contractor_id2 == i)].sort_values(by='Distance').head(5)\n",
    "            l = []\n",
    "            for _, row in a.iterrows():\n",
    "                if row.contractor_id == i:\n",
    "                    l.append(row.contractor_id2)\n",
    "                else:\n",
    "                    l.append(row.contractor_id)\n",
    "\n",
    "            data.append({\n",
    "                \"contractor_id\": i,\n",
    "                \"mean_distance\": mean_distance[i],\n",
    "                \"min_distance\": min_distance[i],\n",
    "                \"max_distance\": max_distance[i],\n",
    "                \"var_distance\": var_distance[i],\n",
    "                \"median_distance\": median_distance[i],\n",
    "                \"top1\": l[0],\n",
    "                \"top2\": l[1],\n",
    "                \"top3\": l[2],\n",
    "                \"top4\": l[3],\n",
    "                \"top5\": l[4],\n",
    "            })\n",
    "        except:\n",
    "            data.append({\n",
    "                \"contractor_id\": i,\n",
    "                \"mean_distance\": 0,\n",
    "                \"min_distance\": 0,\n",
    "                \"max_distance\": 0,\n",
    "                \"var_distance\": 0,\n",
    "                \"median_distance\": 0,\n",
    "                \"top1\": 0,\n",
    "                \"top2\": 0,\n",
    "                \"top3\": 0,\n",
    "                \"top4\": 0,\n",
    "                \"top5\": 0,\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = select_top5_neighbours(full_df1, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contractor_id</th>\n",
       "      <th>mean_distance</th>\n",
       "      <th>min_distance</th>\n",
       "      <th>max_distance</th>\n",
       "      <th>var_distance</th>\n",
       "      <th>median_distance</th>\n",
       "      <th>top1</th>\n",
       "      <th>top2</th>\n",
       "      <th>top3</th>\n",
       "      <th>top4</th>\n",
       "      <th>top5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>438</td>\n",
       "      <td>20.98</td>\n",
       "      <td>12</td>\n",
       "      <td>35</td>\n",
       "      <td>36.42</td>\n",
       "      <td>20.00</td>\n",
       "      <td>656</td>\n",
       "      <td>709</td>\n",
       "      <td>507</td>\n",
       "      <td>801</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>484</td>\n",
       "      <td>547.79</td>\n",
       "      <td>12</td>\n",
       "      <td>32767</td>\n",
       "      <td>16923735.90</td>\n",
       "      <td>24.00</td>\n",
       "      <td>607</td>\n",
       "      <td>321</td>\n",
       "      <td>604</td>\n",
       "      <td>727</td>\n",
       "      <td>532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500</td>\n",
       "      <td>635.33</td>\n",
       "      <td>12</td>\n",
       "      <td>32767</td>\n",
       "      <td>19852958.65</td>\n",
       "      <td>19.00</td>\n",
       "      <td>511</td>\n",
       "      <td>491</td>\n",
       "      <td>626</td>\n",
       "      <td>634</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>615</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>633</td>\n",
       "      <td>473.55</td>\n",
       "      <td>12</td>\n",
       "      <td>32767</td>\n",
       "      <td>14433868.40</td>\n",
       "      <td>26.00</td>\n",
       "      <td>709</td>\n",
       "      <td>683</td>\n",
       "      <td>270</td>\n",
       "      <td>605</td>\n",
       "      <td>243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>18</td>\n",
       "      <td>520.94</td>\n",
       "      <td>12</td>\n",
       "      <td>32767</td>\n",
       "      <td>16183360.16</td>\n",
       "      <td>18.00</td>\n",
       "      <td>267</td>\n",
       "      <td>727</td>\n",
       "      <td>506</td>\n",
       "      <td>592</td>\n",
       "      <td>879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>718</td>\n",
       "      <td>440.34</td>\n",
       "      <td>16</td>\n",
       "      <td>32767</td>\n",
       "      <td>13396562.02</td>\n",
       "      <td>29.00</td>\n",
       "      <td>813</td>\n",
       "      <td>738</td>\n",
       "      <td>771</td>\n",
       "      <td>43</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>631</td>\n",
       "      <td>739.94</td>\n",
       "      <td>12</td>\n",
       "      <td>32767</td>\n",
       "      <td>22996293.67</td>\n",
       "      <td>24.00</td>\n",
       "      <td>776</td>\n",
       "      <td>880</td>\n",
       "      <td>141</td>\n",
       "      <td>353</td>\n",
       "      <td>587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>321</td>\n",
       "      <td>740.03</td>\n",
       "      <td>12</td>\n",
       "      <td>32767</td>\n",
       "      <td>22996137.32</td>\n",
       "      <td>25.00</td>\n",
       "      <td>545</td>\n",
       "      <td>484</td>\n",
       "      <td>231</td>\n",
       "      <td>524</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>395</td>\n",
       "      <td>604.37</td>\n",
       "      <td>12</td>\n",
       "      <td>32767</td>\n",
       "      <td>18850024.79</td>\n",
       "      <td>18.00</td>\n",
       "      <td>709</td>\n",
       "      <td>164</td>\n",
       "      <td>548</td>\n",
       "      <td>438</td>\n",
       "      <td>818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>847 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     contractor_id  mean_distance  min_distance  max_distance  var_distance  \\\n",
       "0              438          20.98            12            35         36.42   \n",
       "1              484         547.79            12         32767   16923735.90   \n",
       "2              500         635.33            12         32767   19852958.65   \n",
       "3              615           0.00             0             0          0.00   \n",
       "4              633         473.55            12         32767   14433868.40   \n",
       "..             ...            ...           ...           ...           ...   \n",
       "842             18         520.94            12         32767   16183360.16   \n",
       "843            718         440.34            16         32767   13396562.02   \n",
       "844            631         739.94            12         32767   22996293.67   \n",
       "845            321         740.03            12         32767   22996137.32   \n",
       "846            395         604.37            12         32767   18850024.79   \n",
       "\n",
       "     median_distance  top1  top2  top3  top4  top5  \n",
       "0              20.00   656   709   507   801   278  \n",
       "1              24.00   607   321   604   727   532  \n",
       "2              19.00   511   491   626   634   455  \n",
       "3               0.00     0     0     0     0     0  \n",
       "4              26.00   709   683   270   605   243  \n",
       "..               ...   ...   ...   ...   ...   ...  \n",
       "842            18.00   267   727   506   592   879  \n",
       "843            29.00   813   738   771    43   200  \n",
       "844            24.00   776   880   141   353   587  \n",
       "845            25.00   545   484   231   524   132  \n",
       "846            18.00   709   164   548   438   818  \n",
       "\n",
       "[847 rows x 11 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:50:49.807728Z",
     "start_time": "2024-10-22T10:50:49.455567Z"
    }
   },
   "outputs": [],
   "source": [
    "full_df1 = full_df1.merge(data, on='contractor_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:56:46.431984Z",
     "start_time": "2024-10-22T10:50:49.811648Z"
    }
   },
   "outputs": [],
   "source": [
    "def top5_neighbours_features(df: pd.DataFrame, graph: pd.DataFrame) -> pd.DataFrame: \n",
    "    d = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        try:\n",
    "            top1 = df[df.contractor_id == row.top1]\n",
    "            top2 = df[df.contractor_id == row.top2]\n",
    "            top3 = df[df.contractor_id == row.top3]\n",
    "            top4 = df[df.contractor_id == row.top4]\n",
    "            top5 = df[df.contractor_id == row.top5]\n",
    "\n",
    "            top1init = top1.contract_init_sum.mean()\n",
    "            top1curr = top1.contract_current_sum.mean()\n",
    "\n",
    "            top2init = top2.contract_init_sum.mean()\n",
    "            top2curr = top2.contract_current_sum.mean()\n",
    "\n",
    "            top3init = top3.contract_init_sum.mean()\n",
    "            top3curr = top3.contract_current_sum.mean()\n",
    "\n",
    "            top4init = top4.contract_init_sum.mean()\n",
    "            top4curr = top4.contract_current_sum.mean()\n",
    "\n",
    "            top5init = top5.contract_init_sum.mean()\n",
    "            top5curr = top5.contract_current_sum.mean()\n",
    "\n",
    "            d.append({\n",
    "                \"top1init\": top1init,\n",
    "                \"top1curr\": top1curr,\n",
    "                \"top2init\": top2init,\n",
    "                \"top2curr\": top2curr,\n",
    "                \"top3init\": top3init,\n",
    "                \"top3curr\": top3curr,\n",
    "                \"top4init\": top4init,\n",
    "                \"top4curr\": top4curr,\n",
    "                \"top5init\": top5init,\n",
    "                \"top5curr\": top5curr\n",
    "            })\n",
    "        except:\n",
    "            d.append({\n",
    "                \"top1init\": 0,\n",
    "                \"top1curr\": 0,\n",
    "                \"top2init\": 0,\n",
    "                \"top2curr\": 0,\n",
    "                \"top3init\": 0,\n",
    "                \"top3curr\": 0,\n",
    "                \"top4init\": 0,\n",
    "                \"top4curr\": 0,\n",
    "                \"top5init\": 0,\n",
    "                \"top5curr\": 0\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42047it [03:12, 218.48it/s]\n"
     ]
    }
   ],
   "source": [
    "data = top5_neighbours_features(full_df1, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:56:47.143646Z",
     "start_time": "2024-10-22T10:56:46.433518Z"
    }
   },
   "outputs": [],
   "source": [
    "full_df1 = pd.concat([full_df1, data], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:56:49.043095Z",
     "start_time": "2024-10-22T10:56:47.148875Z"
    }
   },
   "outputs": [],
   "source": [
    "# fill NaN by mean\n",
    "imputer1 = SimpleImputer(strategy='mean')\n",
    "df_imputed1 = imputer.fit_transform(full_df1)\n",
    "df_imputed1 = pd.DataFrame(df_imputed1, columns=full_df1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:56:49.148Z",
     "start_time": "2024-10-22T10:56:48.997498Z"
    }
   },
   "outputs": [],
   "source": [
    "df_imputed1 = df_imputed1.drop(\n",
    "    columns=['specialization_id', 'project_id', 'building_id', 'contractor_id'])  # drop id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:56:49.508518Z",
     "start_time": "2024-10-22T10:56:49.149099Z"
    }
   },
   "outputs": [],
   "source": [
    "df_imputed1 = df_imputed1.drop(columns=['top1', 'top2', 'top3', 'top4', 'top5'])  # drop top 5 contractor ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:56:49.509601Z",
     "start_time": "2024-10-22T10:56:49.284654Z"
    }
   },
   "outputs": [],
   "source": [
    "X2 = df_imputed1[:len(train)]  # slice by train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:58:44.792065Z",
     "start_time": "2024-10-22T10:56:49.294558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Info] Number of positive: 4704, number of negative: 24127\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022259 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 57627\n",
      "[LightGBM] [Info] Number of data points in the train set: 28831, number of used features: 959\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.163158 -> initscore=-1.634918\n",
      "[LightGBM] [Info] Start training from score -1.634918\n"
     ]
    }
   ],
   "source": [
    "fsmodel1 = FeatureSelectionModel()\n",
    "fsmodel1.fit(X2, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:58:46.602554Z",
     "start_time": "2024-10-22T10:58:44.793828Z"
    }
   },
   "outputs": [],
   "source": [
    "total1 = fsmodel1.get_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:58:46.603840Z",
     "start_time": "2024-10-22T10:58:46.600098Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>contract_current_sum_mean_3M</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>contract_current_sum</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>agg_spark_extended_report__g_contractor__Credi...</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>time</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2093</th>\n",
       "      <td>top3curr</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>building_669</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>agg_cec_requests__g_contract__total_sum_accept...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>building_667</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>building_666</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>agg_BoardOfDirectors__g_contractor__Name__coun...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2098 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                feature  importance\n",
       "178                        contract_current_sum_mean_3M        0.33\n",
       "1                                  contract_current_sum        0.21\n",
       "129   agg_spark_extended_report__g_contractor__Credi...        0.19\n",
       "170                                                time        0.15\n",
       "2093                                           top3curr        0.14\n",
       "...                                                 ...         ...\n",
       "1007                                       building_669        0.00\n",
       "22    agg_cec_requests__g_contract__total_sum_accept...        0.00\n",
       "1005                                       building_667        0.00\n",
       "1004                                       building_666        0.00\n",
       "116   agg_BoardOfDirectors__g_contractor__Name__coun...        0.00\n",
       "\n",
       "[2098 rows x 2 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fsmodel1.fi_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собственно, что и ожидалось. Действительно, показатели соседей очень влияют на дефолт подрядчика.\n",
    "Скорее всего, если у подрядчика большие суммы на его счету, то у него будут средства на материалы и расходы,\n",
    "что увеличивает вероятность того, что он выполнит свой заказ. Видим, что в топе признаков по важности\n",
    "есть фичи, которые мы только что сделали - top1init и top3curr."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем обучить модель без особой предобработки для того, чтобы распределение\n",
    "скора отличалось от моделей с хорошей предобработкой. Так как при ансамблировании\n",
    "этих моделей при разных распределениях, они исправляют ошибки друг друга, что сильно\n",
    "бустит показатель скора. Так как модель с плохой предобработкой, поставим ей небольшой коэфициент."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:58:47.927940Z",
     "start_time": "2024-10-22T10:58:46.608442Z"
    }
   },
   "outputs": [],
   "source": [
    "# fill NaN by mean\n",
    "imputer2 = SimpleImputer(strategy='mean')\n",
    "df_imputed2 = imputer2.fit_transform(full_df2)\n",
    "df_imputed2 = pd.DataFrame(df_imputed2, columns=full_df2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:58:48.024140Z",
     "start_time": "2024-10-22T10:58:47.932315Z"
    }
   },
   "outputs": [],
   "source": [
    "df_imputed2 = df_imputed2.drop(\n",
    "    columns=['specialization_id', 'project_id', 'building_id', 'contractor_id'])  # удаляем id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T10:58:48.028532Z",
     "start_time": "2024-10-22T10:58:48.025117Z"
    }
   },
   "outputs": [],
   "source": [
    "X3 = df_imputed2[:len(train)]  # slice by train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T11:00:38.352311Z",
     "start_time": "2024-10-22T10:58:48.034542Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "[LightGBM] [Info] Number of positive: 4704, number of negative: 24127\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024560 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 54717\n",
      "[LightGBM] [Info] Number of data points in the train set: 28831, number of used features: 944\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.163158 -> initscore=-1.634918\n",
      "[LightGBM] [Info] Start training from score -1.634918\n"
     ]
    }
   ],
   "source": [
    "fsmodel2 = FeatureSelectionModel()\n",
    "fsmodel2.fit(X3, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T11:00:40.550211Z",
     "start_time": "2024-10-22T11:00:38.354520Z"
    }
   },
   "outputs": [],
   "source": [
    "total2 = fsmodel2.get_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T11:00:40.551717Z",
     "start_time": "2024-10-22T11:00:40.547818Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Time series Stemming**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нашем решении мы использовали временные ряды и стемминг\n",
    "Шаги реализации:\n",
    "1. Делаем стемминг на всех данных для получения предсказаний, которые отображают вероятность дефолта определенной записи\n",
    "у подрядчика, не делая глубокий анализ по нему, как бы отражая вероятность дефолта вообщем для таких данных. Значения,\n",
    "полученные со стемминга помогают модели временного ряда давать более точные предсказания, так как без стемминга модель бы\n",
    "давала менее точные предикты в связи с небольшим количеством записей у каждого подрядчика. Модели временного ряда являются\n",
    "узконаправленными. В качестве моделей для стемминга мы использовали RandomForestClassifier и CatBoostClassifier, так как они\n",
    "лучше всего подходят для данного набора данных(В этом ноутбуке есть доказательство этого с помощью библиотеки LazyPredict).\n",
    "Записываем вероятности, полученные со стемминга в коллонку 'score_from_cross_val'.\n",
    "2. Группируем по train датасет contractor_id\n",
    "3. Обучаем свою модель для каждой группы строк, в которой одинаковый contractor_id, имеющий больше 20 записей в\n",
    "трейне(Значение в 20 записей было выбрано для того чтобы избежать неточных результатов от моделей, обученных на менее чем 20 записей)\n",
    "4. Для того чтобы получить предсказания с теста, мы записываем предикты основной модели и колонку 'score_from_cross_val' и предсказываем вероятности дефолта на тестовых данных\n",
    "5. Записываем полученные предикты группы моделей с весом 0,33 и предикты от основной модели с весом 0.67\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T11:27:10.734498Z",
     "start_time": "2024-10-22T11:27:10.691196Z"
    }
   },
   "outputs": [],
   "source": [
    "class TimeSeries:\n",
    "    def __init__(self):\n",
    "        \"\"\"A class for a Time Series Stemming model that will make model-based predictions,\n",
    "        which were trained on data over different periods of time. The class implements the following methods:\n",
    "        fit, predict_proba, submit, init_models, time_series_predict.\n",
    "        \"\"\"\n",
    "        self.rf_model1 = RandomForestClassifier(5000, n_jobs=-1)\n",
    "        self.rf_model2 = RandomForestClassifier(5000, n_jobs=-1)\n",
    "        self.cb_model1 = CatBoostClassifier(2000, verbose=False)\n",
    "        self.cb_model2 = CatBoostClassifier(2000, verbose=False)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit Classification algorithms to X_train and y_train.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like,\n",
    "            Training vectors, where rows is the number of samples\n",
    "            and columns is the number of features.\n",
    "        y : array-like,\n",
    "            Training vectors, where rows is the number of samples\n",
    "            and columns is the number of features.\n",
    "        \"\"\"\n",
    "        self.X = X.copy()\n",
    "        self.y = y.map(int)\n",
    "\n",
    "        self.X['score_from_cross_val'] = [0] * len(X)\n",
    "        self.X1, self.X2, self.y1, self.y2 = train_test_split(self.X, self.y, test_size=0.5, shuffle=False)\n",
    "\n",
    "        self.rf_model1.fit(self.X1, self.y1)\n",
    "        self.rf_model2.fit(self.X2, self.y2)\n",
    "\n",
    "        self.cb_model1.fit(self.X1, self.y1)\n",
    "        self.cb_model2.fit(self.X2, self.y2)\n",
    "\n",
    "    def predict_proba(self):\n",
    "        \"\"\"Prediction probability algorithms to data.\"\"\"\n",
    "        self.p1 = self.rf_model2.predict_proba(self.X1)[:, 1]\n",
    "        self.p2 = self.cb_model2.predict_proba(self.X1)[:, 1]\n",
    "        self.preds1 = self.p1 * 0.8 + self.p2 * 0.2\n",
    "\n",
    "        self.X1['score_from_cross_val'] = self.preds1\n",
    "\n",
    "        self.p3 = self.rf_model1.predict_proba(self.X2)[:, 1]\n",
    "        self.p4 = self.cb_model1.predict_proba(self.X2)[:, 1]\n",
    "        self.preds2 = self.p3 * 0.8 + self.p4 * 0.2\n",
    "\n",
    "        self.X2['score_from_cross_val'] = self.preds2\n",
    "\n",
    "        return pd.concat([self.X1, self.X2])\n",
    "\n",
    "    def submit(self, X):\n",
    "        \"\"\"Prediction probability algorithms to data.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like,\n",
    "            Testing vectors, where rows is the number of samples\n",
    "            and columns is the number of features.\n",
    "        \"\"\"\n",
    "        X1, X2, y1, y2 = train_test_split(X, y, test_size=0.5, shuffle=False)\n",
    "\n",
    "        p1 = self.rf_model2.predict_proba(X1)[:, 1]\n",
    "        p2 = self.cb_model2.predict_proba(X1)[:, 1]\n",
    "        preds1 = p1 * 0.8 + p2 * 0.2\n",
    "\n",
    "        X1['score_from_cross_val'] = preds1\n",
    "\n",
    "        p3 = self.rf_model1.predict_proba(X2)[:, 1]\n",
    "        p4 = self.cb_model1.predict_proba(X2)[:, 1]\n",
    "        preds2 = p3 * 0.8 + p4 * 0.2\n",
    "\n",
    "        X2['score_from_cross_val'] = preds2\n",
    "\n",
    "        return pd.concat([X1, X2])\n",
    "\n",
    "    def init_models(self, data):\n",
    "        \"\"\"Fit Classification algorithms to data.\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : array-like,\n",
    "            Testing vectors, where rows is the number of samples\n",
    "            and columns is the number of features.\n",
    "        \"\"\"\n",
    "        total3 = total + ['contractor_id', 'score_from_cross_val', 'score']\n",
    "        for_each = data[total3].groupby('contractor_id')\n",
    "        self.models = {}\n",
    "        for name, group in tqdm(for_each):\n",
    "            if group.shape[0] > 20:\n",
    "                mod = CatBoostClassifier(allow_const_label=True, verbose=False)\n",
    "                mod.fit(group.drop(columns=['score']), group['score'])\n",
    "                self.models[name] = mod\n",
    "        print(\"DONE\")\n",
    "\n",
    "    def time_series_predict(self, data):\n",
    "        \"\"\"Prediction probability algorithms to data.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like,\n",
    "            Testing vectors, where rows is the number of samples\n",
    "            and columns is the number of features.\n",
    "        \"\"\"\n",
    "        tt = data.groupby('contractor_id')\n",
    "        fn = pd.DataFrame(columns=[\"c\", 'score2'])\n",
    "        count = 0\n",
    "        total_ts = total + ['contractor_id', 'score_from_cross_val']\n",
    "        for name, group in tqdm(tt):\n",
    "            if name in self.models:\n",
    "                ids = group['c']\n",
    "                predd = self.models[name].predict_proba(group[total_ts])[:, 1]\n",
    "                mem = pd.DataFrame({\"c\": ids, 'score2': predd})\n",
    "                fn = pd.concat([fn, mem])\n",
    "                count += 1\n",
    "\n",
    "        submit2 = data.merge(fn, on='c', how=\"left\")\n",
    "        submit2['score'][submit2['score2'].notnull()] = (2 * submit2['score'] + submit2['score2']) / 3\n",
    "        # print(submit2.score2)\n",
    "        return (submit2.drop(columns=['score2', 'c']), submit2)\n",
    "\n",
    "    def save_model(self, path: str):\n",
    "        \"\"\"Model save algorithms.\n",
    "        Parameters\n",
    "        ----------\n",
    "        path : str,\n",
    "            String with path you need to save model.\n",
    "        \"\"\"\n",
    "        joblib.dump(self.models, path)\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, path: str):\n",
    "        \"\"\"Load the models from a file and return an instance of TimeSeries.\"\"\"\n",
    "        instance = cls()\n",
    "        instance.models = joblib.load(path)\n",
    "        print(f\"Models loaded from {path}\")\n",
    "        return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T11:29:29.196658Z",
     "start_time": "2024-10-22T11:27:11.841442Z"
    }
   },
   "outputs": [],
   "source": [
    "ts = TimeSeries()\n",
    "ts.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T11:29:35.328703Z",
     "start_time": "2024-10-22T11:29:29.178818Z"
    }
   },
   "outputs": [],
   "source": [
    "X1 = ts.predict_proba() # dataframe with stemming on score_from_cross_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T11:29:35.332119Z",
     "start_time": "2024-10-22T11:29:35.328702Z"
    }
   },
   "outputs": [],
   "source": [
    "X1['contractor_id'] = contractors1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T11:29:35.922671Z",
     "start_time": "2024-10-22T11:29:35.332327Z"
    }
   },
   "outputs": [],
   "source": [
    "f = X1.copy()\n",
    "f['score'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T11:29:35.925374Z",
     "start_time": "2024-10-22T11:29:35.922729Z"
    }
   },
   "outputs": [],
   "source": [
    "f['contractor_id'] = contractors1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T11:33:38.587622Z",
     "start_time": "2024-10-22T11:29:35.927935Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 646/646 [04:37<00:00,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ts.init_models(f) # fitting models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check contract for valid**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выдвинем гипотезу, что если контрак плохой и его никто не выполняет, то\n",
    "соотвественно, вероятность его выполнения у любого подрядчика будет довольно низка.\n",
    "Проверим это следующим способом. Для каждого контракта будем брать рандомного подрядчика\n",
    "и подставлять ему заместо признаков, связанных с контрактом, признаки контракта, который хотим проверить.\n",
    "Если подрядчик не сможет выполнить эту работу, то скорее всего контракт действительно плохой.\n",
    "Далее сблендим предсказания в соотношении 1:1.\n",
    "Данный алгоритм улучшает предсказания нашей основной модели. Мы группируем train датасет по contract_id и\n",
    "берем среднее у каждого подрядчика. Далее мы проходимся по каждой строчке из теста и находим его ближайшего\n",
    "соседа по графу расстояний между подрядчиками. Далее мы пользуемся проведенным нами анализом данным(у одного\n",
    "контракт айди практически никогда не изменяется вероятность дефолта) и делаем новую строчку, в которой контракт\n",
    "фичи от нынешнего контракта, а контрактор фичи от самого близкого подрядчика. В итоге мы предсказываем вероятность\n",
    "дефолта для новой строчки и записываем в финальные предсказания среднее между вероятностями старой и новой строчки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_norm_contract(copy_train, copy_test1, X_simple, dd, f, simple_model):\n",
    "    data = copy_train.groupby(['contractor_id'], as_index=False).mean()\n",
    "    scores = []\n",
    "    for i in tqdm(range(len(copy_test1))):\n",
    "        if copy_test1.iloc[i, :]['contractor_id'] in dd:\n",
    "            simple_row = copy_test1[X_simple.columns].iloc[[i], :]\n",
    "            contractor_mean_data = data[copy_train.contractor_id == dd[copy_test1.iloc[i, :]['contractor_id']][0]]\n",
    "\n",
    "            row_without_contractor = simple_row.drop(columns=[column for column in simple_row.columns if 'contractor' in column])\n",
    "            contractor_info = contractor_mean_data.filter(like='contractor')\n",
    "            if len(contractor_info) == 0:\n",
    "                scores.append(f.iloc[i][2])\n",
    "            else:\n",
    "                row_without_contractor[contractor_info.columns] = contractor_info.iloc[0]\n",
    "                scores.append((simple_model.predict_proba(row_without_contractor[X_simple.columns])[:, 1][0] + f.iloc[i][2]) / 2)        \n",
    "        else:\n",
    "            scores.append(f.iloc[i][2])\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы протестировать модель на переобучение и понять, как модель работает на других данных, используется перекрестная проверка.\n",
    "Мы выбрали схему GroupKFold по контракту_id\n",
    "GroupKFold — вариант увеличения в k раз, обеспечивающий\n",
    "что одна и та же группа не будет представлена ​​ни в тестовой, ни в обучающей выборке. \n",
    "Например, если данные получены от разных субъектов с использованием нескольких образцов для каждого субъекта,\n",
    "и если модель достаточно гибкая, чтобы учиться на индивидуальных особенностях, \n",
    "невозможно будет распространить его на новые темы. GroupKFold позволяет обнаружить такого рода ситуации переобучения.\n",
    "Каждый предмет проходит разные этапы тестирования, и один и тот же предмет никогда не проходит одновременно и тестирование, и обучение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 378215,
     "status": "ok",
     "timestamp": 1723714446159,
     "user": {
      "displayName": "Sonya Kyshildo",
      "userId": "06895829020768772584"
     },
     "user_tz": -420
    },
    "id": "s9dd6rRcPwOG",
    "outputId": "b7f8c49a-7532-4eb2-ac09-a6944dfb9c2d"
   },
   "outputs": [],
   "source": [
    "n_splt = 6  # number of the folds\n",
    "scores = []\n",
    "recall = []\n",
    "precision = []\n",
    "\n",
    "skf = GroupKFold(n_splits=n_splt)\n",
    "temp = X.copy()\n",
    "\n",
    "for i, (train_index, val_index) in tqdm(enumerate(skf.split(X, y, ctrs))):\n",
    "    train_x, test_x = X.iloc[train_index], X.iloc[val_index]\n",
    "    train_y, test_y = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=5000, n_jobs=-1)\n",
    "    model.fit(train_x[total], train_y)\n",
    "    preds1 = model.predict_proba(test_x[total])[:, 1]\n",
    "\n",
    "    model = CatBoostClassifier(eval_metric='AUC', verbose=False)\n",
    "    model.fit(train_x[total], train_y)\n",
    "    preds2 = model.predict_proba(test_x[total])[:, 1]\n",
    "\n",
    "    preds = preds1 * 0.8 + preds2 * 0.2\n",
    "\n",
    "    scores.append(2 * roc_auc_score(test_y, preds) - 1)\n",
    "    recall.append(recall_score(test_y, preds.round()))\n",
    "    precision.append(precision_score(test_y, preds.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1723714446159,
     "user": {
      "displayName": "Sonya Kyshildo",
      "userId": "06895829020768772584"
     },
     "user_tz": -420
    },
    "id": "s96XOgGxPwOG",
    "outputId": "d6d5968c-c083-462f-a188-bcd44f394128"
   },
   "outputs": [],
   "source": [
    "np.mean(scores)  # gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее сделаем класс модели, так как будем использовать для каждых данных\n",
    "2 модели - RandomForestClassifier и CatBoostClassifier. После этого\n",
    "подберем гиперпараметры и обучим модели, сблендим предсказания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Подбор гиперпараметров**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Попробуем использовать библиотеку LazyClassifier для того, чтобы увидеть, какие модели лучше всего подойдут"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(y_true, y_pred):\n",
    "    return 2 * roc_auc_score(y_true, y_pred) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gini_metric = make_scorer(gini, needs_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_params = {\n",
    "    'iterations': [1000, 2000, 3000, 5000],\n",
    "    'depth': [6, 8, None],\n",
    "    \"random_strength\": [1],\n",
    "    \"verbose\": [0],\n",
    "    \"thread_count\": [-1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    'bootstrap': [True, False],\n",
    "    'criterion': ['gini'],\n",
    "    'max_depth': [None, 10, 15],\n",
    "    'max_features': ['sqrt', None],\n",
    "    'n_estimators': [3000, 5000],\n",
    "    'n_jobs': [-1],\n",
    "    'verbose': [0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_params = { # можно не запускать, дальше записаны лучшие параметры\n",
    "    'iterations': [1000, 2000, 3000],\n",
    "    'l2_leaf_reg': [1, 2, 3, 5, 7],\n",
    "    'leaf_estimation_iterations': [5, 7, 10, 12],\n",
    "    'max_leaves': [60, 61, 62, 63, 64, 65, 70],\n",
    "    'depth': [4, 6, 10, 15],\n",
    "    \"random_strength\": [1],\n",
    "    \"border_count\": [250, 251, 253, 254, 255],\n",
    "    \"verbose\": [0],\n",
    "    \"thread_count\": [-1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_rf = []\n",
    "recall_rf = []\n",
    "precision_rf = []\n",
    "\n",
    "scores_cb = []\n",
    "recall_cb = []\n",
    "precision_cb = []\n",
    "\n",
    "# Разделяем перебор параметров для RandomForest и CatBoost\n",
    "for rf_param in tqdm(ParameterGrid(rf_params)):\n",
    "    temp_scores = []\n",
    "    temp_recall = []\n",
    "    temp_precision = []\n",
    "\n",
    "    for i, (train_index, val_index) in enumerate(skf.split(X, y, ctrs)):\n",
    "        train_x, test_x = X.iloc[train_index], X.iloc[val_index]\n",
    "        train_y, test_y = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        rf_model = RandomForestClassifier(**rf_param)\n",
    "        rf_model.fit(train_x[total], train_y)\n",
    "        preds_rf = rf_model.predict_proba(test_x[total])[:, 1]\n",
    "\n",
    "        temp_scores.append(gini(test_y, preds_rf))\n",
    "        temp_recall.append(recall_score(test_y, preds_rf.round()))\n",
    "        temp_precision.append(precision_score(test_y, preds_rf.round()))\n",
    "\n",
    "    # Вычисляем средние метрики для текущей комбинации параметров RF\n",
    "    scores_rf.append(np.mean(temp_scores))\n",
    "    recall_rf.append(np.mean(temp_recall))\n",
    "    precision_rf.append(np.mean(temp_precision))\n",
    "\n",
    "for cb_param in tqdm(ParameterGrid(cb_params)):\n",
    "    temp_scores = []\n",
    "    temp_recall = []\n",
    "    temp_precision = []\n",
    "\n",
    "    for i, (train_index, val_index) in enumerate(skf.split(X, y, ctrs)):\n",
    "        train_x, test_x = X.iloc[train_index], X.iloc[val_index]\n",
    "        train_y, test_y = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        cb_model = CatBoostClassifier(eval_metric='AUC', **cb_param)\n",
    "        cb_model.fit(train_x[total], train_y)\n",
    "        preds_cb = cb_model.predict_proba(test_x[total])[:, 1]\n",
    "\n",
    "        temp_scores.append(gini(test_y, preds_cb))\n",
    "        temp_recall.append(recall_score(test_y, preds_cb.round()))\n",
    "        temp_precision.append(precision_score(test_y, preds_cb.round()))\n",
    "\n",
    "    scores_cb.append(np.mean(temp_scores))\n",
    "    recall_cb.append(np.mean(temp_recall))\n",
    "    precision_cb.append(np.mean(temp_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_index_rf = np.argmax(scores_rf)\n",
    "best_rf_params = list(ParameterGrid(rf_params))[best_index_rf // len(cb_params)]\n",
    "best_index_cb = np.argmax(scores_cb)\n",
    "best_cb_params = list(ParameterGrid(cb_params))[best_index_cb % len(cb_params)]\n",
    "\n",
    "print(f'Best RF params: {best_rf_params}')\n",
    "print(f'Best CatBoost params: {best_cb_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cb_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model fit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T11:23:52.280776Z",
     "start_time": "2024-10-22T11:23:52.275290Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        \"\"\"Model class, which is based on 2 classification models: RandomForestClassifier and CatBoostClassifier.\n",
    "        The class implements the fit, predict_proba and save_model methods.\n",
    "        \"\"\"\n",
    "        self.best_rf_params = {\n",
    "            'bootstrap': True,\n",
    "            'criterion': 'gini',\n",
    "            'max_depth': None,\n",
    "            'max_features': 'sqrt',\n",
    "            'n_estimators': 5000,\n",
    "            'n_jobs': -1,\n",
    "            'verbose': 0\n",
    "        }\n",
    "        self.best_cb_params = {\n",
    "            'depth': 6,\n",
    "            'iterations': 1000,\n",
    "            'random_strength': 1,\n",
    "            'thread_count': -1,\n",
    "            'verbose': 0\n",
    "        }\n",
    "        self.rf_model = RandomForestClassifier(**self.best_rf_params)\n",
    "        self.cb_model = CatBoostClassifier(**self.best_cb_params)\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series):\n",
    "        \"\"\"Fit Classification algorithms to X_train and y_train.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like,\n",
    "            Training vectors, where rows is the number of samples\n",
    "            and columns is the number of features.\n",
    "        y : array-like,\n",
    "            Training vectors, where rows is the number of samples\n",
    "            and columns is the number of features.\n",
    "        \"\"\"\n",
    "        self.rf_model.fit(X, y.map(int))\n",
    "        self.cb_model.fit(X, y.map(int))\n",
    "\n",
    "    def predict_proba(self, data: pd.DataFrame, class_=1) -> pd.Series:\n",
    "        \"\"\"Prediction probability algorithms to data.\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : array-like,\n",
    "            Testing vectors, where rows is the number of samples\n",
    "            and columns is the number of features.\n",
    "        class_ : int,\n",
    "            Number of class you need to predict\n",
    "        Returns\n",
    "        -------\n",
    "        predictions : Pandas DataFrame\n",
    "            Returns predictions of all the models in a Pandas DataFrame.\n",
    "        \"\"\"\n",
    "        self.preds1 = self.rf_model.predict_proba(data)[:, class_]\n",
    "        self.preds2 = self.cb_model.predict_proba(data)[:, class_]\n",
    "        return self.preds1 * 0.8 + self.preds2 * 0.2\n",
    "\n",
    "    def save_model(self, path_rf: str, path_cb: str):\n",
    "        \"\"\"Model save algorithms.\n",
    "        Parameters\n",
    "        ----------\n",
    "        path : str,\n",
    "            String with path you need to save model.\n",
    "        \"\"\"\n",
    "        joblib.dump(self.rf_model, path_rf)\n",
    "        self.cb_model.save_model(path_cb)\n",
    "    \n",
    "    def load_model(self, path_rf: str, path_cb: str):\n",
    "        \"\"\"Model load algorithms.\n",
    "        Parameters\n",
    "        ----------\n",
    "        path : str,\n",
    "            String with path you need to save model.\n",
    "        \"\"\"\n",
    "        self.rf_model = joblib.load(path_rf)\n",
    "        self.cb_model = CatBoostClassifier()\n",
    "        self.cb_model.load_model(path_cb)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T11:24:46.637932Z",
     "start_time": "2024-10-22T11:23:54.424045Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Model() # proc default6 model\n",
    "model.fit(X[total], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T11:25:29.641018Z",
     "start_time": "2024-10-22T11:24:46.640528Z"
    }
   },
   "outputs": [],
   "source": [
    "model1 = Model() # top 5 neighbours model\n",
    "model1.fit(X2[total1], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T11:26:12.266810Z",
     "start_time": "2024-10-22T11:25:29.634099Z"
    }
   },
   "outputs": [],
   "source": [
    "model2 = Model() # baseline model\n",
    "model2.fit(X3[total2], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T11:26:12.272307Z",
     "start_time": "2024-10-22T11:26:12.267051Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save_model('final/model_rf.joblib', 'final/model_cb.cbm')\n",
    "model1.save_model('final/model_rf1.joblib', 'final/model_cb1.cbm')\n",
    "model2.save_model('final/model_rf2.joblib', 'final/model_cb2.cbm')\n",
    "ts.save_model('final/timeseries.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Koxx25p3PwOH"
   },
   "source": [
    "**Submission**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сблендим предсказания лучших моделей в соотношении 5:2:3.\n",
    "Применим временные ряды и изменим предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T11:26:15.437178Z",
     "start_time": "2024-10-22T11:26:12.272757Z"
    },
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1723714520499,
     "user": {
      "displayName": "Sonya Kyshildo",
      "userId": "06895829020768772584"
     },
     "user_tz": -420
    },
    "id": "ObPSB0AQPwOH"
   },
   "outputs": [],
   "source": [
    "copy_test = test.copy()\n",
    "temp1 = df_imputed[len(train):].reset_index().drop(columns='index')\n",
    "temp2 = df_imputed1[len(train):].reset_index().drop(columns='index')\n",
    "temp3 = df_imputed2[len(train):].reset_index().drop(columns='index')\n",
    "preds1 = model.predict_proba(temp1[total])\n",
    "preds2 = model1.predict_proba(temp2[total1])\n",
    "preds3 = model2.predict_proba(temp3[total2])\n",
    "copy_test = test[['contract_id', 'report_date']]\n",
    "preds = preds1 * 0.5 + preds2 * 0.2 + preds3 * 0.3 # blending predictions\n",
    "copy_test['score'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T11:26:15.653609Z",
     "start_time": "2024-10-22T11:26:15.437219Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 595/595 [00:00<00:00, 1021.46it/s]\n"
     ]
    }
   ],
   "source": [
    "temp1['score_from_cross_val'] = preds # for stemming\n",
    "temp = temp1.reset_index()\n",
    "temp['c'] = range(len(test))\n",
    "temp['contractor_id'] = contractors2\n",
    "temp['score'] = preds\n",
    "s2 = ts.time_series_predict(temp)[0]\n",
    "score = s2.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T11:44:11.211401Z",
     "start_time": "2024-10-22T11:44:11.160214Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contract_id</th>\n",
       "      <th>report_date</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3029</td>\n",
       "      <td>2023-07-30</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4350</td>\n",
       "      <td>2023-07-30</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1095</td>\n",
       "      <td>2023-07-30</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2634</td>\n",
       "      <td>2023-07-30</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6535</td>\n",
       "      <td>2023-07-30</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13211</th>\n",
       "      <td>650</td>\n",
       "      <td>2023-10-29</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13212</th>\n",
       "      <td>4277</td>\n",
       "      <td>2023-10-29</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13213</th>\n",
       "      <td>7316</td>\n",
       "      <td>2023-10-29</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13214</th>\n",
       "      <td>7113</td>\n",
       "      <td>2023-10-29</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13215</th>\n",
       "      <td>541</td>\n",
       "      <td>2023-10-29</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13216 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       contract_id report_date  score\n",
       "0             3029  2023-07-30   0.03\n",
       "1             4350  2023-07-30   0.22\n",
       "2             1095  2023-07-30   0.03\n",
       "3             2634  2023-07-30   0.14\n",
       "4             6535  2023-07-30   0.02\n",
       "...            ...         ...    ...\n",
       "13211          650  2023-10-29   0.12\n",
       "13212         4277  2023-10-29   0.13\n",
       "13213         7316  2023-10-29   0.30\n",
       "13214         7113  2023-10-29   0.17\n",
       "13215          541  2023-10-29   0.17\n",
       "\n",
       "[13216 rows x 3 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = copy_test[['contract_id', 'report_date', 'score']]\n",
    "f['score'] = score\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13216/13216 [02:11<00:00, 100.70it/s]\n"
     ]
    }
   ],
   "source": [
    "scores = check_norm_contract(copy_train, copy_test1, X_simple, dd, f, simple_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_predictions(model, data, top=5):\n",
    "    \"\"\"\n",
    "    Функция для объяснения вероятности предсказания модели CatBoost для нескольких строк.\n",
    "    Параметры:\n",
    "        - model: обученная модель CatBoostClassifier\n",
    "        - data: DataFrame с данными для объяснения\n",
    "    Возвращает:\n",
    "        DataFrame с текстовыми объяснениями вкладов признаков в вероятность предсказания.\n",
    "    \"\"\"\n",
    "    \n",
    "    data_pool = Pool(data)\n",
    "    \n",
    "    shap_values = model.get_feature_importance(type=\"ShapValues\", data=data_pool)\n",
    "    \n",
    "    feature_contributions = shap_values[:, :-1]\n",
    "    predictions = shap_values[:, -1]\n",
    "    \n",
    "    explanations = []\n",
    "    \n",
    "    medians = data.median()\n",
    "    \n",
    "    for i in tqdm(range(len(data))):\n",
    "        feature_info = [\n",
    "            (feature_name, feature_value, feature_contributions[i, j])\n",
    "            for j, (feature_name, feature_value) in enumerate(zip(data.columns, data.iloc[i]))\n",
    "        ]\n",
    "        \n",
    "        if predictions[i] > 0.5:\n",
    "            sorted_features = sorted([f for f in feature_info if f[2] > 0], key=lambda x: abs(x[2]), reverse=True)\n",
    "        else:\n",
    "            sorted_features = sorted([f for f in feature_info if f[2] < 0], key=lambda x: abs(x[2]), reverse=True)\n",
    "        \n",
    "        explanation = []\n",
    "        \n",
    "        for i, (feature_name, feature_value, contribution) in enumerate(sorted_features[:top]):\n",
    "            threshold = medians[feature_name]\n",
    "            sign = '+' if contribution > 0 else '-'\n",
    "            explanation.append(f\"{i+1}) {features_df[features_df.колонка == feature_name].описание.values[0].strip()} значение {feature_value:.3f} {'>' if feature_value > threshold else '<='} чем медиана {threshold} -> {sign}{abs(contribution):.3f} к вероятности\")\n",
    "        \n",
    "        explanations.append(\"\\n\".join(explanation))\n",
    "    \n",
    "    return pd.DataFrame({\"interpretation\": explanations})\n",
    "\n",
    "# Пример использования\n",
    "expl = explain_predictions(model.cb_model, temp1)\n",
    "expl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['interpretation'] = expl.interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.score = pd.Series(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.to_csv('sussubmit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "d2 = f.copy()\n",
    "d2['report_date'] = pd.to_datetime(d2['report_date']).astype('int64') / 10 ** 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2['key'] = list(range(len(d2)))\n",
    "\n",
    "grouped = d2.groupby('contract_id')\n",
    "\n",
    "scores = []\n",
    "\n",
    "for i, row in tqdm(d2.iterrows(), total=len(d2)):\n",
    "    group = grouped.get_group(row['contract_id'])\n",
    "    \n",
    "    group['date_diff'] = (group['report_date'] - row['report_date']).abs()\n",
    "    \n",
    "    group_filtered = group[group['key'] != i]\n",
    "    \n",
    "    if group_filtered.empty:\n",
    "        scores.append(row['score'])\n",
    "    else:\n",
    "        nearest_rows = group_filtered.nsmallest(5, 'date_diff')\n",
    "        \n",
    "        if len(nearest_rows) == 1:\n",
    "            scores.append(nearest_rows.iloc[0]['score'])\n",
    "        else:\n",
    "            avg_score = nearest_rows['score'].mean()\n",
    "            scores.append(avg_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['score_nearest'] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['score'] = (f['score_nearest']+f['score']*0.4)/1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.drop(columns=['score_nearest']).to_csv('sussubmit.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы проделали огромную работу. Проанализировали новые данные,\n",
    "выявили закономерности, поработали с графами и получили score 0.555,\n",
    "что считаем очень хорошим показателем. Далее планируем тюнить параметры наших алгоритмов и моделей,\n",
    "что может еще сильнее увеличить показатели скора, помимо этого, также улучшить алгоритм временных рядов.\n",
    "Мы старались по максимуму комментировать то, что делаем, это должно было сделать код более понятным.\n",
    "Выполняется он без ошибок. Также провели глубокий анализ данных в ноутбуке EDA.ipynb, проверили гипотезы.\n",
    "Создали множество доп. признаков, использовали продвинутые методы для обработки графов, подобрали гиперпараметры для моделей.\n",
    "Большое спасибо, что уделили время и посмотрели наше решение. Хорошего вам дня!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
